{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ZNGOygrFsT"
      },
      "source": [
        "Redes Neuronales para Lenguaje Natural, 2025\n",
        "\n",
        "---\n",
        "# Laboratorio 2\n",
        "\n",
        "En este laboratorio construiremos un sistema de Question Answering (QA) utilizando el método de Retrieval-Augmented Generation (RAG), que implica el uso de un paso de recuperación de información y un paso de generación de respuesta con LLM.\n",
        "\n",
        "**Entrega: 18/11**\n",
        "\n",
        "**Se debe entregar un archivo zip que contenga:**\n",
        "* Este notebook de Python (.ipynb) completo.\n",
        "* Los documentos obtenidos y utilizados como fuentes de información según se explica en la parte 1 (opcionalmente se puede entregar un archivo CSV con los textos de cada documento).\n",
        "* Archivo CSV con el conjunto de preguntas y respuestas como se explica en la parte 5.\n",
        "\n",
        "**No olvidar mantener todas las salidas de cada región de código en el notebook!**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkTcoBzj3OBF"
      },
      "source": [
        "## Parte 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5SUkFIZ3OBJ"
      },
      "source": [
        "### Instalación bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lJHOJf9OVMci"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  COLAB = True\n",
        "else:\n",
        "  print('Not running on CoLab')\n",
        "  COLAB = False\n",
        "\n",
        "#@title Estilo de salida de colab\n",
        "from IPython.display import HTML, display, clear_output\n",
        "if COLAB:\n",
        "    pre_run_cell_fn = lambda: display(HTML('''<style> pre {white-space: pre-wrap;}</style>'''))\n",
        "    get_ipython().events.register('pre_run_cell', pre_run_cell_fn)\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install transformers -U\n",
        "!{sys.executable} -m pip install bitsandbytes\n",
        "!{sys.executable} -m pip install accelerate\n",
        "!{sys.executable} -m pip install sentence-transformers\n",
        "!{sys.executable} -m pip install evaluate\n",
        "!{sys.executable} -m pip install bert_score\n",
        "!{sys.executable} -m pip install google-genai\n",
        "!{sys.executable} -m pip install pymupdf4llm\n",
        "!{sys.executable} -m pip install -qU langchain-text-splitters\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmcDKIFa3OBV"
      },
      "source": [
        "### Creación dataset preguntas y respuestas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RBI30qnv3OBd",
        "outputId": "724aeb28-028e-4838-bfcb-644f3bc28e86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "TEST_DATASET = \"testset.csv\"\n",
        "\n",
        "if not os.path.exists(TEST_DATASET):\n",
        "    with open(TEST_DATASET, \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"question\", \"answer\", \"source\"])\n",
        "\n",
        "        # No relacionadas con el dominio\n",
        "        respuesta_para_pregunta_fuera_de_tema = \"Lo siento, no cuento con información para responder esa pregunta.\"\n",
        "        writer.writerow([\"¿Dónde nace el río Uruguay?\", respuesta_para_pregunta_fuera_de_tema, \"N/A\"])\n",
        "        writer.writerow([\"¿En qué año se firmó el tratado de Tordesillas?\", respuesta_para_pregunta_fuera_de_tema, \"N/A\"])\n",
        "        writer.writerow([\"¿Cuál es el presidente actual de Chile?\", respuesta_para_pregunta_fuera_de_tema, \"N/A\"])\n",
        "        writer.writerow([\"¿Cuál es la capital de Francia?\", respuesta_para_pregunta_fuera_de_tema, \"N/A\"])\n",
        "        writer.writerow([\"¿Quién ganó la Copa del Mundo de la FIFA 2022?\", respuesta_para_pregunta_fuera_de_tema, \"N/A\"])\n",
        "        writer.writerow([\"¿Cuál es la fórmula química del agua?\", respuesta_para_pregunta_fuera_de_tema, \"N/A\"])\n",
        "\n",
        "        # Necesitan información de un chunk\n",
        "        writer.writerow([\"¿Es la Biblioteca Nacional más antigua que la república de Uruguay?\", \"Sí, la Biblioteca Nacional es más antigua que la propia república.\", \"Presentacion.pdf\"])\n",
        "        writer.writerow([\"¿En qué año fue nombrado Francisco Acuña de Figueroa director de la Biblioteca Nacional?\", \"Fue nombrado a mediados de 1840.\", \"Acuña de Figueroa.pdf\"])\n",
        "        writer.writerow([\"¿Qué director de la Biblioteca Nacional es también considerado el primer poeta de la patria?\", \"Francisco Acuña de Figueroa.\", \"Acuña de Figueroa.pdf\"])\n",
        "        writer.writerow([\"¿Qué botánico francés visitó la biblioteca cerrada hacia 1820 y estimó su colección?\", \"Auguste de Saint-Hilaire, quien estimó que el número de libros era de aproximadamente dos mil.\", \"Clausura, expolios, intentos de reapertura.pdf\"])\n",
        "        writer.writerow([\"¿Quién fue el arquitecto que ganó el concurso de 1937 para el actual edificio de la Biblioteca Nacional?\", \"El arquitecto Luis Crespi.\", \"El edificio de la Biblioteca Nacional.pdf\"])\n",
        "        writer.writerow([\"¿Qué director propuso en 1873 la \"\"Creación de una Nueva Biblioteca Nacional\"\" mediante una suscripción popular?\", \"Juan Antonio Tavolara.\", \"La Nueva Biblioteca Nacional.pdf\"])\n",
        "        writer.writerow([\"¿Qué relación tuvieron Dámaso Antonio Larrañaga y Acuña de Figueroa con la Biblioteca Nacional?\", \"Dámaso Antonio Larrañaga fue el protagonista del acto fundacional de la biblioteca. Francisco Acuña de Figueroa fue director de la biblioteca durante siete años, en los complejos tiempos del Sitio Grande de Montevideo.\", \"Presentacion.pdf;Acuña de Figueroa.pdf\"])\n",
        "        writer.writerow([\"¿Qué dos directores de la Biblioteca Nacional tuvieron gestiones criticadas o problemáticas?\", \"La gestión de Francisco Acuña de Figueroa fue polémica por su \"\"camaleonismo político\"\" y su fidelidad a Rivera, que llevó a su destitución. La gestión de Juan Antonio Tavolara fue criticada por rematar obras consideradas \"\"inservibles\"\" a vil precio y Fernández Saldaña la consideró \"\"poco brillante\"\".\", \"Acuña de Figueroa.pdf;La Nueva Biblioteca Nacional.pdf\"])\n",
        "\n",
        "\n",
        "        # Necesitan información de más de un chunk\n",
        "        # <<...la construcción de la sede actual...>> <<...se trasladó...>> <<..el alquiler ... que la biblioteca ocupaba desde 1894,31 en la calle Florida n.º 93.. >> <<...no es solo un edificio ... en 18 de julio..>>\n",
        "        writer.writerow([\"¿La Biblioteca Nacional siempre estuvo en su lugar actual?\", \"No, la sede de la Biblioteca Nacional ha cambiado varias veces a lo largo de la historia.\", \"Presentacion.pdf, La Nueva Biblioteca Nacional.pdf, El edificio de la Biblioteca Nacional.pdf\"])\n",
        "\n",
        "        # <<...actualmente cuenta con 850.000 volúmenes...>> <<..entre 1916 y 1933 se pasó de 59.552 a 132.442 volúmenes...>>\n",
        "        writer.writerow([\"¿En que año la Biblioteca Nacional contó con más volúmenes?\", \"En 2021, al alcanzar los 850.000 volúmenes\", \"La Nueva Biblioteca Nacional.pdf, El edificio de la Biblioteca Nacional.pdf\"])\n",
        "\n",
        "        # <<..fue director por 7 años...>> <<..cuando a mediados de 1840... decidió conferirle el cargo de director...>>\n",
        "        writer.writerow([\"¿En que año terminó el mandato de Francisco Acuña de Figueroa como director de la Biblioteca Nacional?\", \"En 1847\", \"Acuña de Figueroa.pdf\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtgdalArNvHn"
      },
      "source": [
        "## Parte 1: Procesamiento de los documentos\n",
        "\n",
        "En esta parte, cada grupo deberá construir y procesar su conjunto de documentos. Esto consiste de los siguientes pasos:\n",
        "\n",
        "* Elegir un tema dentro de un dominio específico sobre el que trabajar.\n",
        "* Obtener al menos 5 documentos en español que contengan información sobre el tema elegido.\n",
        "* Procesar cada documento para extraer el texto del formato original a un string en Python (por ejemplo, extraer el texto de un PDF).\n",
        "\n",
        "El resultado de esta parte debe ser una lista cargada en memoria que contenga el texto (string) de cada uno de los documentos elegidos.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Se recomienda utilizar artículos de wikipedia para simplificar la etapa de extracción del texto (ver la librería [wikipedia-api](https://github.com/martin-majlis/Wikipedia-API/)).\n",
        "* Opcionalmente puede utilizar documentos PDF, páginas web u otros formatos. En estos casos se sugiere:\n",
        "  * Utilizar la librería PyPDF2 para procesar documentos PDF.\n",
        "  * Utilizar la librería LangChain para procesar páginas web, en particular la clase Html2TextTransformer, que convierte HTML a Markdown ([ejemplo de uso](https://python.langchain.com/v0.2/docs/integrations/document_transformers/html2text/)).\n",
        "* Puede ser conveniente guardar el resultado del procesamiento de los documentos en un archivo CSV (donde cada fila corresponde al texto de un documento) para no tener que repetir este proceso cada vez que se ejecuta el notebook, y en su lugar cargar el archivo CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0RndlIko3OB9",
        "outputId": "3b39e8e1-ade1-4d38-b956-77ef887b193c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pymupdf4llm\n",
        "import csv\n",
        "\n",
        "CORPUS_PATH = \"corpus.csv\"\n",
        "\n",
        "def extractTextFromPdf(file_name):\n",
        "    markdown = pymupdf4llm.to_markdown(file_name)\n",
        "    return markdown\n",
        "\n",
        "def writeIntoCsv(file_name, text):\n",
        "    with open(CORPUS_PATH, \"a\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([file_name, text])\n",
        "\n",
        "def addFileToCsv(file_name):\n",
        "    text = extractTextFromPdf(file_name)\n",
        "    writeIntoCsv(file_name, text)\n",
        "\n",
        "def createCsv():\n",
        "    with open(CORPUS_PATH, \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"file_name\", \"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XuotA7OMHahn",
        "outputId": "8a1e83df-063f-40e9-8e75-b14b25411b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "if not os.path.exists(CORPUS_PATH):\n",
        "  files = [\"Presentacion.pdf\", \"Acunha de Figueroa.pdf\", \"Clausura, expolios, intentos de reapertura.pdf\", \"El edificio de la Biblioteca Nacional.pdf\", \"La Nueva Biblioteca Nacional.pdf\"]\n",
        "\n",
        "  createCsv()\n",
        "  for file_name in files:\n",
        "    addFileToCsv(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qtOV9zSSRul"
      },
      "source": [
        "Los textos resultantes deben estar almacenados en la variable `documents`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iGkUEe10SQrT",
        "outputId": "9dd2a3bb-1694-499e-d2fe-89dad991997c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "documents = pd.read_csv(CORPUS_PATH, header = 0, index_col='file_name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ylFlf6wB3OCm",
        "outputId": "8a4305e4-d56d-49ab-f80e-e8dd62758ce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                             text\n",
            "file_name                                                                                        \n",
            "Presentacion.pdf                                Revista de la Biblioteca Nacional. 17, 5, 2021...\n",
            "Acunha de Figueroa.pdf                          Revista de la Biblioteca Nacional. 17, 71-90, ...\n",
            "Clausura, expolios, intentos de reapertura.pdf  Revista de la Biblioteca Nacional. 17, 53-69, ...\n",
            "El edificio de la Biblioteca Nacional.pdf       Revista de la Biblioteca Nacional. 17, 225-265...\n",
            "La Nueva Biblioteca Nacional.pdf                Revista de la Biblioteca Nacional. 17, 109-117...\n"
          ]
        }
      ],
      "source": [
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCpCYmSrN6oK"
      },
      "source": [
        "## Parte 2: Chunking\n",
        "\n",
        "Una vez que se obtiene el texto de cada documento, se debe realizar la etapa de _chunking_. Esta etapa consiste en dividir cada texto en segmentos más chicos a los que llamamos _chunks_.\n",
        "\n",
        "Realizar la etapa de _chunking_ de forma automática utilizando un método simple que permita obtener _chunks_ de un largo aproximado de 500 caracteres.\n",
        "\n",
        "Puede probar con dividir a nivel de caracteres, palabras o incluso párrafos, teniendo en cuenta que el largo de cada _chunk_ no debería exceder demasiado los 500 caracteres.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Puede utilizar los splitters disponibles en LangChain ([documentación](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)) como RecursiveCharacterTextSplitter, aunque no es obligatorio y también es correcto hacer una implementación propia.\n",
        "* Tener en cuenta que esta etapa es crucial en el resultado final. Cuanto más contextualizados queden los *chunks*, mejor será el rendimiento de la etapa de recuperación de información. Es conveniente minimizar la división de palabras (o párrafos) por la mitad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9VJF4oitZU"
      },
      "source": [
        "#### Definición de splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CMDHuSYjiTG3",
        "outputId": "3e5dd35e-b821-44d3-d116-9c44afced1ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "from enum import StrEnum\n",
        "\n",
        "class SplitterType(StrEnum):\n",
        "    RECURSIVE = \"Recursive\"\n",
        "    MARKDOWN = \"Markdown\"\n",
        "\n",
        "## Recursive Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=450,\n",
        "    chunk_overlap=200, # Con overlaps más chicos el splitter ignora el parámetro en la nueva versión de langchain\n",
        "    length_function=len,\n",
        "    is_separator_regex=False\n",
        ")\n",
        "\n",
        "def splitRecursivo(text):\n",
        "  chunks = text_splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "## Markdown Splitter\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "    (\"####\", \"Header 4\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on,\n",
        "    return_each_line=True\n",
        ")\n",
        "\n",
        "def splitMarkdown(text):\n",
        "  markdown_chunks = markdown_splitter.split_text(text)\n",
        "  chunks = text_splitter.split_documents(markdown_chunks)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FvuZSVJrOu67",
        "outputId": "512d2581-ecc2-40fb-9f24-cbadd8875ac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# incorpora la jerarquía de headers al texto base en el caso del Markdown Splitter\n",
        "def agregar_titulos(chunks):\n",
        "    for chunk in chunks:\n",
        "      header_string = \"\\n\".join(chunk.metadata.values())\n",
        "      final_string = f\"{header_string}\\n{chunk.page_content}\"\n",
        "      chunk.page_content = final_string\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6PgtMbHjVLEj",
        "outputId": "95d5fbe4-1397-4c55-b126-b9cbb3f2c3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def chunk_text(text, splitter=SplitterType.RECURSIVE):\n",
        "\n",
        "  if splitter == SplitterType.RECURSIVE:\n",
        "    chunks = splitRecursivo(text)\n",
        "  elif splitter == SplitterType.MARKDOWN:\n",
        "    chunks = splitMarkdown(text)\n",
        "    chunks = agregar_titulos(chunks)\n",
        "\n",
        "  return chunks # Lista de strings con los chunks del texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cfzD0lmj3OC5",
        "outputId": "e5984510-2f7d-4d3c-ba47-97d781241cd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def chunk_documents(chunk_list, splitter=SplitterType.RECURSIVE):\n",
        "  for document_text in documents['text']:\n",
        "      chunk_list += chunk_text(document_text,splitter=splitter)\n",
        "  if splitter == SplitterType.RECURSIVE:\n",
        "        print(\"Documents chunked using recursive splitter\")\n",
        "  elif splitter == SplitterType.MARKDOWN:\n",
        "        print(\"Documents chunked using markdown splitter\")\n",
        "  return chunk_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JrcaeVeQVg40",
        "outputId": "bd089fc2-f20b-4180-e135-5928d67ab9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents chunked using recursive splitter\n",
            "Documents chunked using markdown splitter\n"
          ]
        }
      ],
      "source": [
        "recursive_chunks = []\n",
        "recursive_chunks = chunk_documents(recursive_chunks, splitter=SplitterType.RECURSIVE)\n",
        "markdown_chunks = []\n",
        "markdown_chunks = chunk_documents(markdown_chunks, splitter=SplitterType.MARKDOWN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ2wgEx3N-O3"
      },
      "source": [
        "## Parte 3: Recuperación de información\n",
        "\n",
        "En esta parte vamos a implementar el método de recuperación de información que nos permitirá obtener los _chunks_ más relevantes para la pregunta.\n",
        "\n",
        "En primer lugar, cargamos el modelo Bi-Encoder que utilizaremos para generar los embeddings utilizando la librería sentence_transformers.\n",
        "\n",
        "Se utiliza el modelo multilingüe [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large), fine-tuning del modelo `xlm-roberta-large` para la tarea de generación de sentence embeddings.\n",
        "\n",
        "Se pueden explorar otros modelos Bi-Encoder, e incluso modelos Cross-Encoder o del tipo ColBERT. En HuggingFace se puede consultar el siguiente [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) que compara varios modelos de este tipo en diferentes tareas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4Fozn2H7vZsj"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_emb = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuNrG2djc3f"
      },
      "source": [
        "A continuación se debe generar las representaciones vectoriales para todos los _chunks_ ([ejemplo de uso](https://huggingface.co/intfloat/multilingual-e5-large#support-for-sentence-transformers)).\n",
        "\n",
        "**Observación:** El modelo que estamos usando espera que los _chunks_ comiencen con el prefijo `passage: ` por lo que será necesario agregarlo al inicio de todos los _chunks_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WeBfJ_Ubjb3k",
        "outputId": "75f65a97-aac6-490f-8ef9-0074cbe9be7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando embeddings para 503 chunks recursivos...\n",
            "Generando embeddings para 742 chunks markdown...\n",
            "Embeddings recursivos generados. Dimensión: (503, 1024)\n",
            "Embeddings markdown generados. Dimensión: (742, 1024)\n"
          ]
        }
      ],
      "source": [
        "def prepare_chunks_for_embedding(chunks):\n",
        "    prepared_chunks = []\n",
        "    i = 0\n",
        "    for chunk in chunks:\n",
        "        # Si es un objeto Document (del markdown splitter), extraer el contenido\n",
        "        if hasattr(chunk, 'page_content'):\n",
        "            text = chunk.page_content\n",
        "        else:\n",
        "            # Si es un string simple\n",
        "            text = chunk\n",
        "\n",
        "        prepared_chunks.append(f\"passage: {text}\")\n",
        "\n",
        "    return prepared_chunks\n",
        "\n",
        "chunks_with_prefix_recursive = prepare_chunks_for_embedding(recursive_chunks)\n",
        "chunks_with_prefix_markdown = prepare_chunks_for_embedding(markdown_chunks)\n",
        "\n",
        "print(f\"Generando embeddings para {len(chunks_with_prefix_recursive)} chunks recursivos...\")\n",
        "chunk_embeddings_recursive = model_emb.encode(chunks_with_prefix_recursive, normalize_embeddings=True)\n",
        "\n",
        "print(f\"Generando embeddings para {len(chunks_with_prefix_markdown)} chunks markdown...\")\n",
        "chunk_embeddings_markdown = model_emb.encode(chunks_with_prefix_markdown, normalize_embeddings=True)\n",
        "\n",
        "print(f\"Embeddings recursivos generados. Dimensión: {chunk_embeddings_recursive.shape}\")\n",
        "print(f\"Embeddings markdown generados. Dimensión: {chunk_embeddings_markdown.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N62kl88KwZ-u"
      },
      "source": [
        "Por último, se debe implementar el algoritmo de búsqueda de los embeddings más cercanos para un embedding dado.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Utilizar la clase NearestNeighbors de sklearn ([documentación](https://scikit-learn.org/dev/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "x5bGq2Etxkj-",
        "outputId": "5dfff10f-8813-4f8a-fcb0-59eec354704f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de vecinos más cercanos recursivo entrenado\n",
            "Modelo de vecinos más cercanos markdown entrenado\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn_model_recursive = NearestNeighbors(\n",
        "    n_neighbors=5,\n",
        "    metric='cosine',\n",
        "    algorithm='brute'\n",
        ")\n",
        "\n",
        "nn_model_recursive.fit(chunk_embeddings_recursive)\n",
        "\n",
        "print(\"Modelo de vecinos más cercanos recursivo entrenado\")\n",
        "\n",
        "nn_model_markdown = NearestNeighbors(\n",
        "    n_neighbors=5,\n",
        "    metric='cosine',\n",
        "    algorithm='brute'\n",
        ")\n",
        "\n",
        "nn_model_markdown.fit(chunk_embeddings_markdown)\n",
        "\n",
        "print(\"Modelo de vecinos más cercanos markdown entrenado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WimFx58x3OD6",
        "outputId": "c3a60d5b-5ffd-439e-da28-44cb0006935b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def retrieve_chunks(all_chunks, nn_model, query, top_k=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        Una tupla (chunks_recuperados, distancias, indices)\n",
        "        - chunks_recuperados: Lista de textos de los chunks más relevantes\n",
        "        - distancias: Distancias coseno a cada chunk\n",
        "        - indices: Índices de los chunks en la lista original\n",
        "    \"\"\"\n",
        "    query_with_prefix = f\"query: {query}\"\n",
        "\n",
        "    query_embedding = model_emb.encode([query_with_prefix], normalize_embeddings=True)\n",
        "\n",
        "    distances, indices = nn_model.kneighbors(query_embedding, n_neighbors=top_k)\n",
        "\n",
        "    # Extraer los chunks correspondientes\n",
        "    chunks_recuperados = []\n",
        "    for idx in indices[0]:\n",
        "        chunk = all_chunks[idx]\n",
        "        # Manejar tanto strings como objetos Document\n",
        "        if hasattr(chunk, 'page_content'):\n",
        "            chunks_recuperados.append(chunk.page_content)\n",
        "        else:\n",
        "            chunks_recuperados.append(chunk)\n",
        "\n",
        "    return chunks_recuperados, distances[0], indices[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwMSqjQUOEzQ"
      },
      "source": [
        "## Parte 4: Generación de respuestas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyPn_Cwtq81r"
      },
      "source": [
        "### Configuración de LLM\n",
        "\n",
        "En esta parte, implementaremos un wrapper flexible que permite experimentar con diferentes modelos de lenguaje:\n",
        "\n",
        "1. **Llama 3.1** (modelo abierto): Utilizaremos el modelo Meta-Llama-3.1-8B-Instruct a través de HuggingFace.\n",
        "2. **Gemini 2.0 Flash** (modelo cerrado): Utilizaremos la API de Google Gemini.\n",
        "\n",
        "Para **Llama 3.1**, es necesario:\n",
        "- Crearse una cuenta de HuggingFace (https://huggingface.co/)\n",
        "- Aceptar los términos para usar el modelo en HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
        "- Crear un token de HuggingFace con permiso de lectura: https://huggingface.co/settings/tokens\n",
        "\n",
        "Para **Gemini**, es necesario:\n",
        "- Obtener una API key de Google AI Studio: https://aistudio.google.com/app/apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dpJk4QSGAPkE",
        "outputId": "57f6a8a5-06a2-42a7-9524-b86df9377d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "83d53e9ad999495883bd4e3fa199d287",
            "d16ba6bb3b604eb6bb75267808de348d",
            "ac72a415a0404c2e8991dcc141ab6d16",
            "9795cc69fe1e44c592dbfed31bcc3f57",
            "c731d50c7eea4080b1596ceeb36e5dc2",
            "cf87165d10024af9b5443cdbd032b5cc",
            "67f913f53fa8413eb8834e5e50f01d93",
            "da71fb63c7aa4b149e26608ab74fee3c",
            "d37d54b66f144e71b6f30042365e00ba",
            "c8fe10375e04437fb1eb744f46ddaa66",
            "b66c003a50424625a99e390f8ba999d2",
            "6cff492b263d4480b542f50c1dd0276e",
            "c0222a8798814c7c964b52ee624bd388",
            "e702c1fede1d42c687366459149da1e3",
            "0ba25f8bc8bf46f186792818243333ca",
            "44e33d076cfc4abc94d900306c645b5c",
            "eb33740635c345398e3dbb64f75b8f79",
            "fec2699074084f6eba92926deaf38349",
            "7e122beccc8e46a0b90df90ad57f6b29",
            "1ffe75359f7d4cd0b773f9071c6c3733"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83d53e9ad999495883bd4e3fa199d287"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Ejecutar para conectarse a HuggingFace (solo necesario para Llama 3.1)\n",
        "from huggingface_hub import notebook_login, login\n",
        "\n",
        "if COLAB:\n",
        "    notebook_login()\n",
        "else:\n",
        "    token = ''\n",
        "    login(token=token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fPqX6DH1Gzn"
      },
      "source": [
        "### Definición del Wrapper de LLM\n",
        "\n",
        "A continuación se define una clase abstracta que permite intercambiar fácilmente entre diferentes modelos de lenguaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LVPu5MUiTBog",
        "outputId": "9458c5e2-5ccb-47c3-fe3d-7ad6f840642b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "class LLMWrapper(ABC):\n",
        "    \"\"\"Clase base abstracta para wrappers de modelos de lenguaje.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, prompt: str, temperature: float = 0.0, max_tokens: int = 500) -> str:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_model_name(self) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class LlamaWrapper(LLMWrapper):\n",
        "    \"\"\"Wrapper para el modelo Llama 3.1 de Meta vía HuggingFace.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "        import torch\n",
        "\n",
        "        print(\"Inicializando Llama 3.1...\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "\n",
        "        if COLAB:\n",
        "            # Configuración de cuantización a 4 bits (para mejorar eficiencia)\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                quantization_config=bnb_config,\n",
        "                device_map=\"auto\")\n",
        "        else:\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "                device_map=\"auto\")\n",
        "\n",
        "        print(\"Llama 3.1 inicializado correctamente\")\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 0.0, max_tokens: int = 500) -> str:\n",
        "        from transformers import GenerationConfig, pipeline\n",
        "\n",
        "        # Configuración de temperatura\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=temperature if temperature > 0 else None,\n",
        "            do_sample=temperature > 0)\n",
        "\n",
        "        # Inicializar pipeline para generación de texto\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            config=generation_config,\n",
        "            tokenizer=self.tokenizer,\n",
        "            pad_token_id=self.tokenizer.eos_token_id)\n",
        "\n",
        "        # Generar texto\n",
        "        output = pipe(\n",
        "            prompt,\n",
        "            return_full_text=False,\n",
        "            max_new_tokens=max_tokens)\n",
        "\n",
        "        return output[0]['generated_text']\n",
        "\n",
        "    def get_model_name(self) -> str:\n",
        "        return \"Llama-3.1-8B-Instruct\"\n",
        "\n",
        "\n",
        "class GeminiWrapper(LLMWrapper):\n",
        "    \"\"\"Wrapper para el modelo Gemini 2.0 Flash de Google.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        from google import genai\n",
        "\n",
        "        print(\"Inicializando Gemini 1.5 Flash...\")\n",
        "\n",
        "        if api_key is None:\n",
        "            api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "        if api_key is None:\n",
        "            print(\"No se encontró la API key de Gemini en las variables de entorno.\")\n",
        "            api_key = getpass(\"Por favor, ingrese su API key de Gemini: \")\n",
        "            # Guardar en variables de entorno para esta sesión\n",
        "            os.environ[\"GEMINI_API_KEY\"] = api_key\n",
        "\n",
        "        self.client = genai.Client(api_key=api_key)\n",
        "\n",
        "        print(\"Gemini 2.0 Flash inicializado correctamente\")\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 0.0, max_tokens: int = 500) -> str:\n",
        "        from google.genai import types\n",
        "\n",
        "        config = types.GenerateContentConfig(\n",
        "            temperature=temperature if temperature > 0 else 0.0,\n",
        "            max_output_tokens=max_tokens)\n",
        "\n",
        "        # Generar respuesta\n",
        "        response = self.client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=prompt,\n",
        "            config=config)\n",
        "\n",
        "        return response.text\n",
        "\n",
        "    def get_model_name(self) -> str:\n",
        "        return \"Gemini-2.0-Flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuDEy7jh3OEL"
      },
      "source": [
        "### Instanciar modelos\n",
        "\n",
        "Seleccionar qué modelo(s) se va a inicializar. Se puede inicializar ambos para facilitar la experimentación posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k9t3C5S3OEM"
      },
      "outputs": [],
      "source": [
        "llama_model = LlamaWrapper()\n",
        "gemini_model = GeminiWrapper()\n",
        "\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGESsQX1k5_"
      },
      "source": [
        "### Función auxiliar para generación de respuestas\n",
        "\n",
        "Esta función utiliza el modelo activo seleccionado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Jyf3eQPB4zQz",
        "outputId": "d5c4f1e1-8e9e-44f0-e0d1-635badf9912d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_response(\n",
        "    prompt: str,\n",
        "    model: LLMWrapper = None,\n",
        "    temp: float = 0.0,\n",
        "    max_tok: int = 500\n",
        ") -> str:\n",
        "    if model is None:\n",
        "        model = active_model\n",
        "\n",
        "    return model.generate(prompt, temperature=temp, max_tokens=max_tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN9wyhSm5Mtj"
      },
      "source": [
        "### Crear prompt y generar respuesta\n",
        "\n",
        "Escribir la función `create_prompt(question, use_chat_template=True)` que dada una pregunta, genere la prompt que se utilizará para generar la respuesta. Tener en cuenta que se debe realizar la búsqueda semántica de los _chunks_ más cercanos a la pregunta utilizando lo implementado en la parte 3.\n",
        "\n",
        "**Observación:** Al igual que para los _chunks_, el modelo Bi-Encoder espera que la pregunta comience con un prefijo especial: `query: ` por lo que será necesario agregarlo al inicio de la pregunta para generar el embedding.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Puede probar con distintas cantidades de _chunks_ recuperados, pero se sugiere comenzar con 3. Tener en cuenta que más _chunks_ recuperados y agregados en la prompt implica mayor uso de memoria en inferencia.\n",
        "* El parámetro `use_chat_template` permite controlar si se aplica el template de chat de Llama (necesario para Llama 3.1, opcional para Gemini). Para Llama usar `True`, para Gemini se puede probar con `True` o `False` según el formato que se prefiera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xQtVcehH-nQJ",
        "outputId": "fc412c1a-523c-4452-d65d-7ef44008c228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def create_prompt(question, active_model, use_chat_template=True, model_for_template=None, top_k=3, chunk_list=recursive_chunks, nn_model=nn_model_recursive):\n",
        "    \"\"\"\n",
        "    Crea el prompt para el modelo de lenguaje incluyendo contexto recuperado.\n",
        "\n",
        "    Args:\n",
        "        question: La pregunta del usuario\n",
        "        use_chat_template: Si True, aplica el template de chat de Llama\n",
        "        model_for_template: Modelo del cual usar el tokenizer (solo para Llama)\n",
        "        top_k: Número de chunks a recuperar (por defecto 3)\n",
        "\n",
        "    Returns:\n",
        "        El prompt formateado\n",
        "    \"\"\"\n",
        "\n",
        "    retrieved_chunks, _, _ = retrieve_chunks(chunk_list, nn_model, question, top_k=top_k)\n",
        "\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    system_message = \"\"\"Eres un asistente experto en responder preguntas basándote únicamente en el contexto proporcionado.\n",
        "\n",
        "Instrucciones:\n",
        "- Si la información para responder la pregunta está en el contexto, proporciona una respuesta clara y concisa.\n",
        "- Si la información NO está en el contexto, responde: \"Lo siento, no cuento con información para responder esa pregunta.\"\n",
        "- No inventes información que no esté en el contexto.\n",
        "- Responde en español.\"\"\"\n",
        "\n",
        "    user_message = f\"\"\"Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta: {question}\"\"\"\n",
        "\n",
        "    if use_chat_template:\n",
        "        if model_for_template is None and isinstance(active_model, LlamaWrapper):\n",
        "            model_for_template = active_model\n",
        "\n",
        "        if isinstance(model_for_template, LlamaWrapper):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ]\n",
        "            prompt = model_for_template.tokenizer.apply_chat_template(\n",
        "                messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "        else:\n",
        "            prompt = f\"{system_message}\\n\\n{user_message}\"\n",
        "    else:\n",
        "        prompt = f\"{system_message}\\n\\n{user_message}\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfYv8FZ2-0fR"
      },
      "source": [
        "### Análisis de chunks obtenidos para ciertas preguntas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"¿Quién fue el arquitecto que ganó el concurso de 1937 para el actual edificio de la Biblioteca Nacional?\"\n",
        "retrieved_chunks, _, _ = retrieve_chunks(recursive_chunks, nn_model_recursive, question, top_k=3)\n",
        "\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "print(\"RecursiveSplitter\")\n",
        "print(context)\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "retrieved_chunks, _, _ = retrieve_chunks(markdown_chunks, nn_model_markdown, question, top_k=3)\n",
        "\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "print(\"MarkdownSplitter\")\n",
        "print(context)"
      ],
      "metadata": {
        "id": "WUsnQmuD0MWd",
        "outputId": "250df419-e321-46d9-c3e4-59fe3fd057c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveSplitter\n",
            "propia idea de lo que significa una biblioteca nacional y cómo esto\n",
            "se tradujo en arquitectura. Se examinan así proyectos anteriores para\n",
            "la sede de la biblioteca, las vicisitudes del concurso de 1937, las modificaciones hechas por su autor, el arquitecto Luis Crespi, en 1942\n",
            "y las valoraciones contemporáneas y posteriores a su inauguración.\n",
            "\n",
            "La idea de vincular los libros y la lectura, entendidos como\n",
            "motores del «perfeccionamiento espiritual», a una dimensión\n",
            "sagrada, no era una novedad en octubre de 1937, cuando el arquitecto Luis Crespi [3] entregó el anteproyecto que, a la postre,\n",
            "sería el germen del actual edificio de la Biblioteca Nacional (BN).\n",
            "Existían numerosos casos de bibliotecas, tanto históricas como\n",
            "contemporáneas, que aludían, de diversas maneras, a los templos\n",
            "\n",
            "**El concurso (1937)**\n",
            "\n",
            "\n",
            "Quince anteproyectos se presentaron al concurso para Biblioteca\n",
            "Nacional y Museo Histórico Nacional (MHN), reservado, como era\n",
            "usual, a arquitectos nacionales. Este doble programa era, en realidad,\n",
            "provisorio:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MarkdownSplitter\n",
            "**El edificio de la Biblioteca Nacional**\n",
            "**Un templo para los libros:** **la sede de la Biblioteca Nacional**\n",
            "Santia o Medero [1] **g**\n",
            "celebrado en Montevideo en marzo de 1940. [15] Horacio Acosta y\n",
            "Lara y Leopoldo Carlos Agorio fueron, según los datos que poseemos, dos de los miembros del jurado en el concurso (el primero fue\n",
            "su presidente [16] y el segundo fue elegido por los concursantes [17] ). El\n",
            "prestigio del que gozaban en 1937 está fuera de toda duda.\n",
            "\n",
            "**El edificio de la Biblioteca Nacional**\n",
            "**Un templo para los libros:** **la sede de la Biblioteca Nacional**\n",
            "Santia o Medero [1] **g**\n",
            "La comunidad de arquitectos también apoyó la resolución del\n",
            "jurado. La biblioteca fue uno de los edificios uruguayos que se\n",
            "expuso durante el quinto Congreso Panamericano de Arquitectos\n",
            "celebrado en Montevideo en marzo de 1940. [15] Horacio Acosta y\n",
            "Lara y Leopoldo Carlos Agorio fueron, según los datos que poseemos, dos de los miembros del jurado en el concurso (el primero fue\n",
            "\n",
            "**El edificio de la Biblioteca Nacional**\n",
            "**Un templo para los libros:** **la sede de la Biblioteca Nacional**\n",
            "Santia o Medero [1] **g**\n",
            "**El concurso (1937)**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"¿En que año la Biblioteca Nacional contó con más volúmenes?\"\n",
        "retrieved_chunks, _, _ = retrieve_chunks(recursive_chunks, nn_model_recursive, question, top_k=3)\n",
        "\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "print(\"RecursiveSplitter\")\n",
        "print(context)\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "retrieved_chunks, _, _ = retrieve_chunks(markdown_chunks, nn_model_markdown, question, top_k=3)\n",
        "\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "print(\"MarkdownSplitter\")\n",
        "print(context)"
      ],
      "metadata": {
        "id": "ZOM-aZ-f1uZF",
        "outputId": "0e7618a2-7426-40a6-a102-a7590f8b591f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveSplitter\n",
            "En 1870, según el inventario reproducido por Adolfo Vaillant en\n",
            "su «Anuario y Almanaque», la Biblioteca Nacional contaba 3.653\n",
            "volúmenes, 970 folletos, 337 tomos de diarios encuadernados y 179\n",
            "de diarios sin encuadernar, « _cifras miserables que denuncian el estado_\n",
            "_de abandono o de crisis del establecimiento_ » [15] a juicio de Acevedo.\n",
            "\n",
            "39. Un informe indica que el volumen del acervo casi se triplicó entre 1916 y 1933. Se pasó\n",
            "de 37.398 obras en 59552 volúmenes y folletos a un total de 91.767 obras en 132.442 volúmenes, folletos y hojas sueltas, 235 grabados, 271 mapas y planos y 617 piezas musicales.\n",
            "En 1932 concurrieron a la BN 58.966 lectores. AABN. Libro Asuntos, 1933, vol. 1. Asunto\n",
            "7292, folio 46. En 1935, la cantidad de lectores había aumentado a 75.982. AABN. Libro\n",
            "\n",
            "En 1867, la Biblioteca Nacional se trasladó a la planta alta de la\n",
            "Casa de Correos (inaugurada en ese mismo año), lo que dio lugar\n",
            "a que su director mandara a remate público obras consideradas «in­\n",
            "servibles» (« _que ni para pasto de la polilla servían_ »), algunas de ellas\n",
            "vendidas a 10 centésimos el tomo según _El Siglo,_ y donara otras\n",
            "duplicadas a la Universidad de la República (500 volúmenes) y a\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MarkdownSplitter\n",
            "**El edificio de la Biblioteca Nacional**\n",
            "**Un templo para los libros:** **la sede de la Biblioteca Nacional**\n",
            "Santia o Medero [1] **g**\n",
            "39. Un informe indica que el volumen del acervo casi se triplicó entre 1916 y 1933. Se pasó\n",
            "de 37.398 obras en 59552 volúmenes y folletos a un total de 91.767 obras en 132.442 volúmenes, folletos y hojas sueltas, 235 grabados, 271 mapas y planos y 617 piezas musicales.\n",
            "En 1932 concurrieron a la BN 58.966 lectores. AABN. Libro Asuntos, 1933, vol. 1. Asunto\n",
            "7292, folio 46. En 1935, la cantidad de lectores había aumentado a 75.982. AABN. Libro\n",
            "\n",
            "**José Antonio Tavolara** **y la «Nueva Biblioteca Nacional»**\n",
            "Gustavo Toledo [1]\n",
            "Hacia 1873, el establecimiento dirigido por Tavolara había alcan\n",
            "zado 8.136 volúmenes, 3.844 folletos y 499 tomos de diarios. Y la\n",
            "Memoria del año siguiente indicaba que el número de personas que\n",
            "había concurrido a su sala ascendía a 7.366 lectores y que el Museo,\n",
            "que funcionaba en el mismo edificio y bajo la misma dirección,\n",
            "había sido visitado por 32.827 personas [16] .\n",
            "\n",
            "**José Antonio Tavolara** **y la «Nueva Biblioteca Nacional»**\n",
            "Gustavo Toledo [1]\n",
            "Al asumir el cargo, Tavolara se encontró que el establecimiento\n",
            "contaba con 6.643 volúmenes, 2.800 folletos y 162 colecciones de\n",
            "diarios, pero –según testimonia en su memoria anual–\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = '¿En que año terminó el mandato de Francisco Acuña de Figueroa como director de la Biblioteca Nacional?'\n",
        "retrieved_chunks, _, _ = retrieve_chunks(recursive_chunks, nn_model_recursive, question, top_k=3)\n",
        "\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "print(\"RecursiveSplitter\")\n",
        "print(context)\n",
        "print('\\n\\n\\n')\n",
        "\n",
        "retrieved_chunks, _, _ = retrieve_chunks(markdown_chunks, nn_model_markdown, question, top_k=3)\n",
        "\n",
        "context = \"\\n\\n\".join(retrieved_chunks)\n",
        "print(\"MarkdownSplitter\")\n",
        "print(context)"
      ],
      "metadata": {
        "id": "GwG3AFyJ2sLG",
        "outputId": "e36e93c8-1314-4c53-8a74-e16390fcba05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveSplitter\n",
            "Cuando a mediados de 1840 el gobierno oriental presidido por\n",
            "el general Fructuoso Rivera decidió conferirle el cargo de director de\n",
            "la Biblioteca Nacional, Francisco Acuña de Figueroa era no solo el\n",
            "más importante poeta de la joven República, sino también, por lejos,\n",
            "el más polémico. En la agitada década anterior, Acuña de Figueroa\n",
            "había ganado toda la atención popular cuando en 1833 el presidente\n",
            "\n",
            "Revista de la Biblioteca Nacional. 17, 71-90, 2021. ISSN 0797-9061\n",
            "\n",
            "\n",
            "asamblea el 22 setiembre de 1847 a favor del destierro del caudillo.\n",
            "Acuña de Figueroa demostró su fidelidad a Rivera al no participar de\n",
            "la asamblea. Los colorados no riveristas tomaron apunte del hecho y\n",
            "lo destituyeron de la dirección de la Biblioteca Nacional.\n",
            "\n",
            "Luego de que dejó la dirección de la Biblioteca Nacional, Acuña\n",
            "de Figueroa siguió escribiendo en mil formas diferentes, sobre mil\n",
            "temas distintos, con estilo vivaz. Alabó a todos los gobernantes de\n",
            "turno, hasta su muerte en 1862. Escribió Gallinal: «Fue nuestro pri­\n",
            "mer ‘hombre de letras’. ¡Curioso destino el suyo! Cantor de la Patria,\n",
            "a la que había negado tres veces en las horas trágicas del amanecer.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MarkdownSplitter\n",
            "**Acuña de Figueroa,** **el director que pedía indulgencia**\n",
            "Valentín Trujillo [1]\n",
            "Cuando a mediados de 1840 el gobierno oriental presidido por\n",
            "el general Fructuoso Rivera decidió conferirle el cargo de director de\n",
            "la Biblioteca Nacional, Francisco Acuña de Figueroa era no solo el\n",
            "más importante poeta de la joven República, sino también, por lejos,\n",
            "el más polémico. En la agitada década anterior, Acuña de Figueroa\n",
            "había ganado toda la atención popular cuando en 1833 el presidente\n",
            "\n",
            "**Acuña de Figueroa,** **el director que pedía indulgencia**\n",
            "Valentín Trujillo [1]\n",
            "En 1837, Oribe había nombrado la dirección de la Biblioteca\n",
            "Nacional a una comisión encabezada por Tomás Vilardebó, e in\n",
            "tegrada además por Dámaso Antonio Larrañaga, Ramón Massini,\n",
            "Bernardo Prudencio Berri, Manuel Errazquin y Cristóbal Salvañach.\n",
            "Esta comisión, que debió tener un trabajo arduo de reorganización\n",
            "y salvaguarda de la menguada colección, vino a sustituir, a su vez, a\n",
            "una anterior nombrada en 1833, comandada entonces por Massini\n",
            "\n",
            "**Acuña de Figueroa,** **el director que pedía indulgencia**\n",
            "Valentín Trujillo [1]\n",
            "El responsable debió lidiar con algunos problemas prácticos de la\n",
            "Biblioteca Nacional, que luego se extendieron a lo largo de las déca\n",
            "das, como por ejemplo las dificultades en el manejo del personal. Un\n",
            "ejemplo es el informe que Acuña de Figueroa elevó a Vidal en mayo\n",
            "de 1841, donde le describió la ausencia de un portero y la sugerencia\n",
            "de alguien para sustituirlo:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYOvIEHAOJSc"
      },
      "source": [
        "## Parte 5: Evaluación\n",
        "A continuación vamos a evaluar la solución construida. Para ello, se deben seguir los siguientes pasos:\n",
        "\n",
        "* Construir un conjunto de evaluación de forma manual que contenga al menos 12 preguntas y respuestas con las siguientes características:\n",
        "  * Al menos 3 preguntas deben necesitar información presente en más de un _chunk_ para ser respondidas correctamente.\n",
        "  * Al menos 3 preguntas no deben estar relacionadas con el dominio, y su respuesta de referencia debe ser algo similar a: \"Lo siento, no cuento con información para responder esa pregunta.\"\n",
        "* El conjunto debe estar en un archivo CSV llamado testset.csv, con las columnas \"question\" y \"answer\".\n",
        "\n",
        "Se deberá realizar al menos tres experimentos diferentes y evaluar sobre el mismo conjunto de test con la métrica BERTScore. Los experimentos deben variar en al menos uno de los siguientes elementos:\n",
        "* Método de chunking\n",
        "* Modelo (o método) de retrieval\n",
        "* Modelo de generación (LLM)\n",
        "* Método de prompting (se puede probar con few-shot, chain of thought, etc)\n",
        "* Otros aspectos que considere relevantes a probar\n",
        "\n",
        "A continuación se definen funciones auxiliares para la evaluación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "JkFBE_iEKkkm",
        "outputId": "2d9a0452-edce-428d-8329-e292b97bf171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def generate_predictions(\n",
        "    questions,\n",
        "    model=None,\n",
        "    use_chat_template=True,\n",
        "    use_delay=False,\n",
        "    chunk_list=recursive_chunks,\n",
        "    nn_model=nn_model_recursive\n",
        "):\n",
        "    if model is None:\n",
        "        model = active_model\n",
        "\n",
        "    predictions = []\n",
        "    for question in tqdm(questions, desc=f\"Generando con {model.get_model_name()}\"):\n",
        "        prompt = create_prompt(question, model, use_chat_template=use_chat_template, chunk_list=chunk_list, nn_model=nn_model)\n",
        "        prediction = get_response(prompt, model=model)\n",
        "        if use_delay:\n",
        "            time.sleep(10)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def evaluate_predictions(predictions, references, experiment_name=\"\"):\n",
        "    \"\"\"Evalúa predicciones usando BERTScore.\"\"\"\n",
        "    bertscore = evaluate.load(\"bertscore\")\n",
        "    results = bertscore.compute(predictions=predictions, references=references, lang='es')\n",
        "\n",
        "    metrics = {\n",
        "        'precision': np.array(results['precision']).mean(),\n",
        "        'recall': np.array(results['recall']).mean(),\n",
        "        'f1': np.array(results['f1']).mean()\n",
        "    }\n",
        "\n",
        "    if experiment_name:\n",
        "        print(f\"\\n=== Resultados: {experiment_name} ===\")\n",
        "    print(f\"BERTScore P:  {metrics['precision']:.3f}\")\n",
        "    print(f\"BERTScore R:  {metrics['recall']:.3f}\")\n",
        "    print(f\"BERTScore F1: {metrics['f1']:.3f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "F-wIITRgKKoE",
        "outputId": "4b9a841e-49a6-4cd2-e49f-960e1226703c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"testset.csv\")\n",
        "\n",
        "questions = df[\"question\"].tolist()\n",
        "references = df[\"answer\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8quL9-CVKEB"
      },
      "source": [
        "Evalúe los experimentos realizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ig-5IFZvlnhE",
        "outputId": "d0a8e42b-26b3-45be-c98f-a488d95c0403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import json\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d2iv_r_izcY"
      },
      "outputs": [],
      "source": [
        "# Evaluar experimentos\n",
        "# Almacenar resultados para comparación posterior\n",
        "results_dict = {}\n",
        "\n",
        "experimentos = [\n",
        "    {\n",
        "        \"numero\": 1,\n",
        "        \"nombre\": \"Exp1: Llama 3.1 con chunking recursivo\",\n",
        "        \"modelo\": llama_model,\n",
        "        \"chunking\": recursive_chunks,\n",
        "        \"nn_model\": nn_model_recursive\n",
        "    },\n",
        "    {\n",
        "        \"numero\": 2,\n",
        "        \"nombre\": \"Exp2: Llama 3.1 con chunking markdown\",\n",
        "        \"modelo\": llama_model,\n",
        "        \"chunking\": markdown_chunks,\n",
        "        \"nn_model\": nn_model_markdown\n",
        "    },\n",
        "    {\n",
        "        \"numero\": 3,\n",
        "        \"nombre\": \"Exp3: Gemini 2.0 Flash con chunking markdown\",\n",
        "        \"modelo\": gemini_model,\n",
        "        \"chunking\": markdown_chunks,\n",
        "        \"nn_model\": nn_model_markdown\n",
        "    }\n",
        "]\n",
        "\n",
        "respuestas = []\n",
        "evaluaciones = []\n",
        "for experimento in experimentos:\n",
        "    modelo = experimento[\"modelo\"]\n",
        "    chunking = experimento[\"chunking\"]\n",
        "    nearest_neighbors = experimento[\"nn_model\"]\n",
        "\n",
        "    respuesta = generate_predictions(questions, model=modelo, use_chat_template=True, chunk_list=chunking, nn_model=nearest_neighbors)\n",
        "    respuestas.append({ f\"experimento{experimento['numero']}\": respuesta })\n",
        "    evaluacion = evaluate_predictions(respuesta, references, experimento[\"nombre\"])\n",
        "    evaluaciones.append({ f\"experimento{experimento['numero']}\": evaluacion })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluaciones)"
      ],
      "metadata": {
        "id": "ALA0zs_ernFN",
        "outputId": "fe584da2-63eb-434c-ebc4-da8b4c6ad138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'experimento1': {'precision': np.float64(0.8302036348511191), 'recall': np.float64(0.8368074929012972), 'f1': np.float64(0.831717287792879)}}, {'experimento2': {'precision': np.float64(0.8580391091458938), 'recall': np.float64(0.8581515796044293), 'f1': np.float64(0.857164740562439)}}, {'experimento3': {'precision': np.float64(0.83458612946903), 'recall': np.float64(0.8652663721757776), 'f1': np.float64(0.8488758942660164)}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for respuesta in respuestas[0]['experimento1']:\n",
        "  print(respuesta)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "for respuesta in respuestas[1]['experimento2']:\n",
        "  print(respuesta)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "for respuesta in respuestas[2]['experimento3']:\n",
        "  print(respuesta)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vjN1QaKZvJRq",
        "outputId": "471cd963-4d34-4a1e-dd49-26eea40e15bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style> pre {white-space: pre-wrap;}</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Sí, según el texto, la Biblioteca Nacional es más antigua que la república de Uruguay.\n",
            "En el año 1840.\n",
            "Francisco Acuña de Figueroa es considerado el primer poeta de la patria y también fue director de la Biblioteca Nacional.\n",
            "Auguste de Saint-Hilaire.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Dámaso Larrañaga fundó la Biblioteca Nacional en 1816.\n",
            "Los dos directores de la Biblioteca Nacional cuya gestión tuvo problemas o fue criticada, según el contexto proporcionado, fueron:\n",
            "\n",
            "1. Tomás Vilardebó (en 1837): Fue parte de la comisión encabezada por él, que debió enfrentar la reorganización y la salvaguarda de la colección de la Biblioteca Nacional.\n",
            "2. Francisco Acuña de Figueroa: En el texto se menciona que en 1841, en un informe a Vidal, describió problemas como la ausencia de un portero y la necesidad de encontrar a alguien para sustituirlo.\n",
            "No, la Biblioteca Nacional no siempre estuvo en su lugar actual.\n",
            "Según el contexto, la Biblioteca Nacional contó con más volúmenes en 1933, con un total de 132.442 volúmenes.\n",
            "No cuento con información para responder esa pregunta.\n",
            "\n",
            "\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "Sí, según el contexto, la Biblioteca Nacional es más antigua que la república de Uruguay.\n",
            "En 1833.\n",
            "Francisco Acuña de Figueroa.\n",
            "Auguste de Saint-Hilaire.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "José Antonio Tavolara.\n",
            "Dámaso Antonio Larrañaga formó parte de la comisión encabezada por Tomás Vilardebó que se encargó de la reorganización y salvaguarda de la colección de la Biblioteca Nacional en 1837.\n",
            "Los dos directores de la Biblioteca Nacional que tuvieron gestiones criticadas o problemáticas mencionados en el contexto son:\n",
            "\n",
            "1. Francisco Acuña de Figueroa\n",
            "2. Tomás Vilardebó\n",
            "No, la Biblioteca Nacional no siempre estuvo en su lugar actual. Según el texto, el edificio actual de la Biblioteca Nacional es el resultado de una serie de traslados a diferentes ubicaciones, que comenzaron desde 1816 y continuaron hasta 1894.\n",
            "Según el contexto, en 1933 la Biblioteca Nacional contaba con 132.442 volúmenes.\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "Sí, la Biblioteca Nacional es más antigua que la propia república.\n",
            "\n",
            "Francisco Acuña de Figueroa fue nombrado director de la Biblioteca Nacional en julio de 1840.\n",
            "\n",
            "Francisco Acuña de Figueroa, considerado el primer poeta de la patria, fue director de la Biblioteca Nacional.\n",
            "\n",
            "Auguste de Saint-Hilaire visitó la biblioteca cerrada hacia finales de 1820 y estimó su colección.\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "José Antonio Tavolara.\n",
            "\n",
            "Dámaso Antonio Larrañaga integró la comisión encabezada por Tomás Vilardebó en 1837 para la dirección de la Biblioteca Nacional. Francisco Acuña de Figueroa solicitó el empleo de bibliotecario público y fue nombrado director de la Biblioteca Nacional a mediados de 1840.\n",
            "\n",
            "El texto menciona que Acuña de Figueroa debió lidiar con problemas prácticos en la Biblioteca Nacional, como dificultades en el manejo del personal. También menciona que la comisión encabezada por Tomás Vilardebó tuvo un trabajo arduo de reorganización y salvaguarda de la menguada colección. Por lo tanto, Acuña de Figueroa y Tomás Vilardebó tuvieron gestiones criticadas o problemáticas.\n",
            "\n",
            "No, los espacios que había ocupado previamente a este edificio fueron: el Fuerte de Montevideo (1816-1859), donde ocupaba una pieza en la planta alta, un edificio en la calle Sarandí y Misiones (1859-1878), un ala del Teatro Solís (1879, donde continuó funcionando el Museo Nacional de Historia Natural). En 1880 se muda al edificio del Correo mencionado (1880-1889), luego a la calle Soriano 96-98 (1889-1894).\n",
            "\n",
            "En 1933, la Biblioteca Nacional contaba con 132.442 volúmenes, folletos y hojas sueltas.\n",
            "\n",
            "Lo siento, no cuento con información para responder esa pregunta.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1GKFIKIVk42"
      },
      "source": [
        "### Reporte de resultados\n",
        "\n",
        "Reportar los resultados obtenidos en los experimentos realizados completando la siguiente tabla:\n",
        "\n",
        "| Exp | Descripción | P BERTScore | R BERTScore | F BERTScore |\n",
        "|-----|-------------|-------------|-------------|-------------|\n",
        "| 1 | Llama 3.1 RecursiveSplitter| 0.830| 0.837| 0.832 |\n",
        "| 2 | Llama 3.1 MarkdownSplitter| 0.858| 0.858 |0.857 |\n",
        "| 3 | Gemini 2.0 Flash MarkdownSplitter | 0.835| 0.865 | 0.849 |\n",
        "\n",
        "Responda las siguientes preguntas:\n",
        "\n",
        "1. Explique brevemente las diferencias en los experimentos realizados, ¿Qué aspectos se varió en el pipeline de RAG?\n",
        "\n",
        "2. ¿Son consistentes los resultados obtenidos con lo que esperaba?\n",
        "\n",
        "3. ¿Le parece que la métrica BERTScore está capturando correctamente las diferencias de los distintos experimentos realizados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peHQat3XWzX2"
      },
      "source": [
        "**Pregunta 1**\n",
        "\n",
        "Se realizaron variaciones en el método de chunking y el modelo generativo. No se probaron variaciones en el modelo de embeddings (multilingual-e5-large) ni el algoritmo de recuperación de información (k-NN con k=5). Se buscó variar solo un aspecto a la vez para poder evaluar el efecto de éste sobre la performance dentro de lo posible (no tenemos control sobre el no-determinismo propio de los LLMs).\n",
        "1. En el primer experimento se utilizó el splitter Recursivo de Langchain para realizar el chunking y Llama 3.1 como modelo generativo.\n",
        "2. En el segundo experimento se utilizó el splitter Markdown de Langchain y luego nuevamente Llama 3.1 como modelo generativo.\n",
        "3. Finalmente, en el último experimento se mantuvo el splitter Markdown de Langchain y se utilizó Gemini-2.0-Flash como modelo generativo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 2**\n",
        "\n",
        "Los resultados superaron las expectativas, principalmente para la capacidad de los modelos de embeddings de recuperar el contexto (los chunks) adecuados para cada pregunta. Llama la atención como con un pipeline simple se pueden obtener resultados relativamente buenos. Probablemente puedan ser mejorados con una mejor etapa de recuperación de información."
      ],
      "metadata": {
        "id": "Q23WwenNtxQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pregunta 3**\n"
      ],
      "metadata": {
        "id": "gmzD1tHl34ER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el problema planteado, las respuestas generadas por los modelos de los experimentos no tenían grandes diferencias entre sí, y la métrica BERTScore reflejó esta similitud.\n",
        "No sé presentó una situación que resaltara ni las limitaciones de esta métrica, ni las diferencias entre los experimentos, probablemente por la forma estructurada de responder cuándo no sé contaba con información en el contexto."
      ],
      "metadata": {
        "id": "7G3Cd8Z84GWl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBiXjU79Iy1T"
      },
      "source": [
        "**Evaluación Humana**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV8KYfBmizcY"
      },
      "source": [
        "Además de las métricas utilizadas en el reporte de resultado, resulta de interés evaluar el dataset de preguntas de test de forma manual, para poder contrastar y visualizar los matices de las mismas.\n",
        "\n",
        "\n",
        "| Exp | Descripción | Preguntas No Relacionadas | Preguntas (1 chunk) | Preguntas (2+ chunks) |\n",
        "|-----|-------------|-------------|-------------|-------------|\n",
        "| 1 | Llama 3.1 RecursiveSplitter| 6/6 | 5/8 * | 1/3 |\n",
        "| 2 | Llama 3.1 MarkdownSplitter| 6/6 | 5/8 * | 0.5/3 * |\n",
        "| 3 | Gemini 2.0 Flash MarkdownSplitter| 6/6 | 6.5/8 * | 0.5/3 * |\n",
        "\n",
        "El peor desempeño se observó consistentemente en las preguntas para las cuáles se precisaba más de un chunk para poder responderlas.\n",
        "Una de ellas, la referida a en qué año se contó con más volúmenes, implicaba cierto razonamiento (encontrar todas las menciones de volúmenes y años y elegir el mayor). Tenía cierta trampa porque en un chunk se compara la cantidad de volúmenes entre dos años, pero en otro chunk se menciona una cantidad de volúmenes mayor.\n",
        "Al analizar los chunks recuperados para la misma, se observó que para ningún splitter se obtenían los adecuados. Esto probablemente se deba a que el chunk que contaba con la información de en que año se tuvó más volumenes tenía la palabra \"Actualmente\", y para inferir para que año refiere debía extaer información sobre la fecha de publicación al principio del documento.\n",
        "Esta fue una preguntada pensada para evaluar la capacidad de recuperación de contexto de los splitters.\n",
        "\n",
        "La pregunta referida al año del final del mandato como director de Acuña de Figueroa también implicaba cierto \"razonamiento\", ya que se menciona un año de comienzo en un chunk, y una duración del mandato en otro chunk.\n",
        "Al analizar el contexto obtenido, nuevamente se observa que los títulos del markdown agregan ruido. El chunk que contenía la duración del mandato\n",
        "no fue recuperado, y con la información recuperada (varias fechas de momentos relacionados a Figueroa) no era posible inferir el año final del mandato.\n",
        "\n",
        "Finalmente, la pregunta sobre la ubicación de la Biblioteca fue respondida \"correctamente\" por dos de los tres experimentos ya que en ambos se respondió \"No, \", pero los detalles provistos a continuación son inexactos e incompletos.\n",
        "De estas observaciones surge la cuestión de qué significa que una pregunta sea respondida correctamente, por lo cuál a algunas respuestas le otorgamos medio punto de correctitud, marcado con un * en la tabla de resultados.\n",
        "\n",
        "Tal vez se precise un mayor procesamiento de los textos y del contexto obtenido como más relevante para cada pregunta.\n",
        "\n",
        "Hubo una pregunta particular que todos los experimentos fallaron al responderla (arquitecto /  Luis Crespi). Al observar cuáles habían sido el contexto recuperado para la misma, se observó que en al usar el RecursiveSplitter se recuperaban chunks relacionados a la pregunta pero insuficientes para inferir la respuesta, mientras que para el MarkdownSplitter los chunks obtenidos directamente no estaban relacionados a la pregunta, debido a que la información extra añadida por los títulos terminaba \"contaminando\" o agregando ruido a la búsqueda de contexto.\n",
        "\n",
        "Las preguntas no relacionadas fueron respondidas correctamente como fuera de dominio por los tres experimentos. Considerando que el RAG no pudo obtener o encontrar respuestas en el texto a preguntas dentro del dominio, no es un indicador demasiado significativo. Por lo menos es rescatable el hecho de que se respeto la directiva de utilizar solo la información del contexto, aunque seguro era información que se encontraba en su memoria paramétrica.  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FtgdalArNvHn",
        "gCpCYmSrN6oK",
        "XJ2wgEx3N-O3",
        "qwMSqjQUOEzQ",
        "LYOvIEHAOJSc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83d53e9ad999495883bd4e3fa199d287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_67f913f53fa8413eb8834e5e50f01d93"
          }
        },
        "d16ba6bb3b604eb6bb75267808de348d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da71fb63c7aa4b149e26608ab74fee3c",
            "placeholder": "​",
            "style": "IPY_MODEL_d37d54b66f144e71b6f30042365e00ba",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ac72a415a0404c2e8991dcc141ab6d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c8fe10375e04437fb1eb744f46ddaa66",
            "placeholder": "​",
            "style": "IPY_MODEL_b66c003a50424625a99e390f8ba999d2",
            "value": ""
          }
        },
        "9795cc69fe1e44c592dbfed31bcc3f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6cff492b263d4480b542f50c1dd0276e",
            "style": "IPY_MODEL_c0222a8798814c7c964b52ee624bd388",
            "value": true
          }
        },
        "c731d50c7eea4080b1596ceeb36e5dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e702c1fede1d42c687366459149da1e3",
            "style": "IPY_MODEL_0ba25f8bc8bf46f186792818243333ca",
            "tooltip": ""
          }
        },
        "cf87165d10024af9b5443cdbd032b5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44e33d076cfc4abc94d900306c645b5c",
            "placeholder": "​",
            "style": "IPY_MODEL_eb33740635c345398e3dbb64f75b8f79",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "67f913f53fa8413eb8834e5e50f01d93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "da71fb63c7aa4b149e26608ab74fee3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37d54b66f144e71b6f30042365e00ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8fe10375e04437fb1eb744f46ddaa66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b66c003a50424625a99e390f8ba999d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cff492b263d4480b542f50c1dd0276e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0222a8798814c7c964b52ee624bd388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e702c1fede1d42c687366459149da1e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba25f8bc8bf46f186792818243333ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "44e33d076cfc4abc94d900306c645b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb33740635c345398e3dbb64f75b8f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fec2699074084f6eba92926deaf38349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e122beccc8e46a0b90df90ad57f6b29",
            "placeholder": "​",
            "style": "IPY_MODEL_1ffe75359f7d4cd0b773f9071c6c3733",
            "value": "Connecting..."
          }
        },
        "7e122beccc8e46a0b90df90ad57f6b29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffe75359f7d4cd0b773f9071c6c3733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}