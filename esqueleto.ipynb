{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f4e04b48",
      "metadata": {
        "id": "f4e04b48"
      },
      "source": [
        "# Esqueleto: Freesound Audio Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd61a3ee",
      "metadata": {
        "id": "cd61a3ee"
      },
      "source": [
        "---\n",
        "## 1. Configuración del Entorno"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "281c3260",
      "metadata": {
        "id": "281c3260"
      },
      "source": [
        "### 1.1 Instalación de Librerías y Montaje del Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee27dd4e",
      "metadata": {
        "id": "ee27dd4e"
      },
      "outputs": [],
      "source": [
        "# Para el registro de experimentos\n",
        "!pip install -q comet_ml\n",
        "!pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "08bce52b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08bce52b",
        "outputId": "dcc33b5d-28f5-4dc1-9bff-1e21a31767f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Montar Google Drive para acceder a los datos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a33d1c",
      "metadata": {
        "id": "b2a33d1c"
      },
      "source": [
        "### 1.2 Imports y Configuración de Semillas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "44fb03eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44fb03eb",
        "outputId": "45bb8444-0e2f-437a-86b4-9f08aea29858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.18.0\n",
            "Num GPUs Available: 1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "import comet_ml\n",
        "from google.colab import userdata\n",
        "\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7304061d",
      "metadata": {
        "id": "7304061d"
      },
      "source": [
        "### 1.3 Configuración de Constantes y Rutas del Proyecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2f06ed4a",
      "metadata": {
        "id": "2f06ed4a"
      },
      "outputs": [],
      "source": [
        "# --- Rutas del Dataset ---\n",
        "# DRIVE_PROJECT = pathlib.Path('path/a/los/datos/') # Varía para cada uno, el mío es:\n",
        "DRIVE_PROJECT = pathlib.Path('/content/drive/MyDrive/Colab Notebooks/')\n",
        "BASE_PATH = DRIVE_PROJECT\n",
        "PATH_CURATED = BASE_PATH / 'train_curated'\n",
        "PATH_TEST = BASE_PATH / 'test'\n",
        "\n",
        "PATH_NOISY = BASE_PATH / 'train_noisy'\n",
        "DF_NOISY_PATH = BASE_PATH / 'train_noisy.csv'\n",
        "DF_NOISY_MULTIHOT_PATH = BASE_PATH / 'train_noisy_multihot.csv'\n",
        "TFRECORD_NOISY_DIR = BASE_PATH / 'tfrecords_noisy'\n",
        "\n",
        "# --- Archivos CSV ---\n",
        "DF_CURATED_PATH = BASE_PATH / 'train_curated.csv'\n",
        "\n",
        "DF_SUBMISSION_PATH = BASE_PATH / 'sample_submission_v24.csv'\n",
        "VOCABULARY_PATH = BASE_PATH / ('vocabulary.csv')\n",
        "\n",
        "# --- Parámetros de Audio y Espectrogramas ---\n",
        "# Remuestreamos a 16kHz para aligerar el procesamiento.\n",
        "# Los clips originales están a 44.1kHz\n",
        "SAMPLE_RATE = 16000\n",
        "WINDOW_LENGTH_SECONDS = 1\n",
        "WINDOW_STEP_SECONDS = 0.5\n",
        "\n",
        "# Parámetros para Mel-Spectrogram\n",
        "N_FFT = 1024       # Longitud de la ventana FFT\n",
        "HOP_LENGTH = 256   # Salto entre ventanas\n",
        "N_MELS = 128       # Número de bandas Mel (común)\n",
        "FMIN = 20          # Frecuencia mínima\n",
        "FMAX = SAMPLE_RATE / 2 # Frecuencia máxima\n",
        "\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3239171a",
      "metadata": {
        "id": "3239171a"
      },
      "source": [
        "---\n",
        "## 2. Análisis Exploratorio de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16fa3506",
      "metadata": {
        "id": "16fa3506"
      },
      "source": [
        "### 2.1 Carga de Metadatos y Vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f51d121",
      "metadata": {
        "id": "6f51d121"
      },
      "outputs": [],
      "source": [
        "# Leemos los archivos CSV que contienen los nombres de archivo y las etiquetas\n",
        "#df_curated = pd.read_csv(DF_CURATED_PATH)\n",
        "df_noisy = pd.read_csv(DF_NOISY_PATH)\n",
        "# df_submission = pd.read_csv(DF_SUBMISSION_PATH)\n",
        "\n",
        "# Mapear strings a índices\n",
        "df_vocab = pd.read_csv(VOCABULARY_PATH, header=None, names=['id', 'label'])\n",
        "LABELS = df_vocab['label'].tolist()\n",
        "NUM_CLASSES = len(LABELS)\n",
        "label_string_to_label_index = {string: index for index, string in enumerate(LABELS)}\n",
        "label_index_to_label_string = {index: string for index, string in enumerate(LABELS)}\n",
        "\n",
        "print(f\"Número total de clases: {NUM_CLASSES}\")\n",
        "print(\"Ejemplo de etiquetas:\", LABELS[:5])\n",
        "\n",
        "#print('Curated:')\n",
        "#print(df_curated.head())\n",
        "print('\\nNoisy:')\n",
        "print(df_noisy.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7527f8",
      "metadata": {
        "id": "4b7527f8"
      },
      "source": [
        "### 2.2 Visualización de distribución de clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94743b4",
      "metadata": {
        "id": "d94743b4"
      },
      "outputs": [],
      "source": [
        "\"\"\"Convierte las etiquetas de string a un formato multi-hot-encoding.\"\"\"\n",
        "def process_labels(df, label_to_id_map):\n",
        "    df['labels_list'] = df['labels'].str.split(',')\n",
        "\n",
        "    # Creamos la matriz de one-hot encoding\n",
        "    labels_matrix = np.zeros((len(df), len(label_to_id_map)), dtype=np.float32)\n",
        "    for i, row in enumerate(df['labels_list']):\n",
        "        for label in row:\n",
        "            if label in label_to_id_map:\n",
        "                labels_matrix[i, label_to_id_map[label]] = 1.0\n",
        "\n",
        "    return labels_matrix\n",
        "\n",
        "# Aplicamos la función a nuestros dataframes\n",
        "# Hay que usarla también en los noisy luego\n",
        "y_noisy = process_labels(df_noisy, label_string_to_label_index)\n",
        "\n",
        "print(\"Forma de las etiquetas del conjunto curado:\", y_noisy.shape)\n",
        "print(y_noisy[0])\n",
        "\n",
        "plt.figure(figsize=(15, 20))\n",
        "class_counts = pd.Series(y_noisy.sum(axis=0), index=LABELS).sort_values(ascending=True)\n",
        "plt.barh(class_counts.index, class_counts.values)\n",
        "plt.title('Distribución de Clases en el Conjunto Noisy')\n",
        "plt.xlabel('Número de Ocurrencias')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7516bbe5",
      "metadata": {
        "id": "7516bbe5"
      },
      "source": [
        "### 2.4 Visualización de un Ejemplo: Audio y Espectrograma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "395e9a58",
      "metadata": {
        "id": "395e9a58"
      },
      "source": [
        "Convert Audio Tensor to Mel Spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c68f2ee",
      "metadata": {
        "id": "1c68f2ee"
      },
      "outputs": [],
      "source": [
        "def plot_spectogram(log_mel_spectrogram, frame_step, sample_rate):\n",
        "    log_mel_spectrogram_np = log_mel_spectrogram.numpy()\n",
        "    # Calculate time axis for proper x-axis scaling\n",
        "    total_frames = log_mel_spectrogram_np.shape[0]\n",
        "    time_duration = (total_frames * frame_step) / sample_rate\n",
        "\n",
        "    # Plot the mel spectrogram\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.imshow(log_mel_spectrogram_np.T, aspect='auto', origin='lower',\n",
        "                extent=[0, time_duration, FMIN, FMAX])\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title('Mel Spectrogram')\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Frequency (Hz)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1367c3eb",
      "metadata": {
        "id": "1367c3eb"
      },
      "source": [
        "Convert Audio Tensor to MFCSS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a57c67",
      "metadata": {
        "id": "26a57c67"
      },
      "outputs": [],
      "source": [
        "def convert_audio_tensor_to_mfccs(audio_tensor, sample_rate, frame_length=N_FFT, frame_step=HOP_LENGTH, window_fn=tf.signal.hann_window, plot=True):\n",
        "    spectogram = convert_audio_tensor_to_mel_spectogram(audio_tensor, sample_rate, frame_length=frame_length, frame_step=frame_step, window_fn=window_fn, plot=plot)\n",
        "    mfcss = tf.signal.mfccs_from_log_mel_spectrograms(spectogram)[..., :13]\n",
        "    if plot:\n",
        "        mfcss_np = mfcss.numpy()\n",
        "        # Calculate time axis for proper x-axis scaling\n",
        "        total_frames = mfcss_np.shape[0]\n",
        "        time_duration = (total_frames * frame_step) / sample_rate\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.imshow(mfcss_np.T, aspect='auto', origin='lower',\n",
        "                   extent=[0, time_duration, 0, mfcss_np.shape[1]])\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title('MFCCs')\n",
        "        plt.xlabel('Time (s)')\n",
        "        plt.ylabel('MFCC')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return mfcss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd8fe37",
      "metadata": {
        "id": "bbd8fe37"
      },
      "outputs": [],
      "source": [
        "def plot_wavelength(audio, sample_rate):\n",
        "    # Visualizar\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # Waveform\n",
        "    time_axis = np.arange(len(audio)) / sample_rate\n",
        "    plt.plot(time_axis, audio)\n",
        "    plt.title(\"Forma de Onda (Waveform)\")\n",
        "    plt.ylabel(\"Amplitud\")\n",
        "    plt.xlabel(\"Tiempo (s)\")\n",
        "    plt.xlim(0,15)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8b688f8",
      "metadata": {
        "id": "b8b688f8"
      },
      "source": [
        "Load Sample, convert to MFCSS and display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bca79c",
      "metadata": {
        "id": "a0bca79c"
      },
      "outputs": [],
      "source": [
        "def load_sample_and_convert_to_mfccs(filepath='sample.wav', sample_rate=22050):\n",
        "    audio, sample_rate = load_audio_with_librosa(filepath, sample_rate)\n",
        "    audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
        "\n",
        "    plot_wavelength(audio, sample_rate)\n",
        "    mfcss = convert_audio_tensor_to_mfccs(audio_tensor, sample_rate)\n",
        "    display(Audio(audio, rate=sample_rate))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "892c413a",
      "metadata": {
        "id": "892c413a"
      },
      "source": [
        "Plot Audio and Spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d983760a",
      "metadata": {
        "id": "d983760a"
      },
      "outputs": [],
      "source": [
        "def plot_audio_and_spectrogram(filepath, sr=SAMPLE_RATE):\n",
        "    # Cargar audio\n",
        "    audio_binary = tf.io.read_file(str(filepath))\n",
        "    waveform, original_sr = tf.audio.decode_wav(audio_binary)\n",
        "    waveform = tf.squeeze(waveform, axis=-1)\n",
        "\n",
        "    # Remuestrear (tf.signal no tiene resample, se puede usar librosa o asumir pre-procesado)\n",
        "    # Por simplicidad acá, asumimos que los archivos ya están a TARGET_SR o que esta función se adapta.\n",
        "    # Después se podría usar la librería librosa para el remuestreo.\n",
        "    print(f\"Frecuencia de muestreo original: {original_sr.numpy()} Hz\")\n",
        "\n",
        "    # Calcular espectrograma Mel\n",
        "    stfts = tf.signal.stft(waveform, frame_length=N_FFT, frame_step=HOP_LENGTH, fft_length=N_FFT)\n",
        "    spectrograms = tf.abs(stfts)\n",
        "\n",
        "    mel_spectrograms = tf.tensordot(\n",
        "        spectrograms,\n",
        "        tf.signal.linear_to_mel_weight_matrix(N_MELS, spectrograms.shape[-1], sr, FMIN, FMAX),\n",
        "        1)\n",
        "\n",
        "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
        "\n",
        "    # Visualizar\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
        "\n",
        "    # Waveform\n",
        "    time_axis = np.arange(len(waveform)) / original_sr.numpy()\n",
        "    axes[0].plot(time_axis, waveform.numpy())\n",
        "    axes[0].set_title(\"Forma de Onda (Waveform)\")\n",
        "    axes[0].set_ylabel(\"Amplitud\")\n",
        "\n",
        "    # Spectrogram\n",
        "    img = axes[1].imshow(tf.transpose(log_mel_spectrograms).numpy(),\n",
        "                         aspect='auto', origin='lower',\n",
        "                         extent=[time_axis.min(), time_axis.max(), FMIN, FMAX])\n",
        "    axes[1].set_title(\"Espectrograma Mel (Log)\")\n",
        "    axes[1].set_xlabel(\"Tiempo (s)\")\n",
        "    axes[1].set_ylabel(\"Frecuencia (Hz)\")\n",
        "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Reproducir audio\n",
        "    display(Audio(waveform.numpy(), rate=original_sr.numpy()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7f7689",
      "metadata": {
        "id": "af7f7689"
      },
      "source": [
        "Plot one sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3468777",
      "metadata": {
        "id": "e3468777"
      },
      "outputs": [],
      "source": [
        "# Tomamos un ejemplo del conjunto curado\n",
        "sample_row = df_noisy.iloc[10]\n",
        "sample_path = PATH_NOISY / sample_row['fname']\n",
        "print(f\"Archivo: {sample_row['fname']}\")\n",
        "print(f\"Etiquetas: {sample_row['labels']}\")\n",
        "#plot_audio_and_spectrogram(sample_path)\n",
        "load_sample_and_convert_to_mfccs(sample_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "304e8050",
      "metadata": {
        "id": "304e8050"
      },
      "source": [
        "---\n",
        "## 3. Preprocesamiento y Pipeline de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1e7ef22",
      "metadata": {
        "id": "c1e7ef22"
      },
      "source": [
        "### 3.0 Conversión de labels a multihot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9495b77",
      "metadata": {
        "id": "b9495b77"
      },
      "source": [
        "Parse Label string and Encode to multihot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cd89c690",
      "metadata": {
        "id": "cd89c690"
      },
      "outputs": [],
      "source": [
        "def encode_label_string_as_multihot(labels_string, num_classes=NUM_CLASSES, label_string_to_label_index=label_string_to_label_index):\n",
        "    assert label_string_to_label_index is not None\n",
        "\n",
        "    \"\"\"Convert '0,2,3' to [1,0,1,1,0,...]\"\"\"\n",
        "    multihot = [0.0] * num_classes\n",
        "\n",
        "    try:\n",
        "        split_labels_strings = labels_string.split(',')\n",
        "\n",
        "        clean_labels = [label.strip() for label in split_labels_strings]\n",
        "\n",
        "        for label_string in clean_labels:\n",
        "            label_index = label_string_to_label_index[label_string]\n",
        "            if 0 <= label_index < num_classes:\n",
        "                multihot[label_index] = 1.0\n",
        "            else:\n",
        "                print(f\"Warning: Label {label_index} out of range [0, {num_classes-1}]\")\n",
        "\n",
        "        return np.array(multihot, dtype=np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing labels '{labels_string}': {e}\")\n",
        "        return np.array(multihot, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "06bb492a",
      "metadata": {
        "id": "06bb492a"
      },
      "outputs": [],
      "source": [
        "def encode_labels_to_multihot(data_frame_string_labels, num_classes=NUM_CLASSES, label_string_to_label_index=label_string_to_label_index):\n",
        "    data_frame_multihot_labels = data_frame_string_labels.copy()\n",
        "    data_frame_multihot_labels['labels'] = data_frame_string_labels['labels'].apply(\n",
        "        lambda label_string: encode_label_string_as_multihot(label_string, num_classes=num_classes, label_string_to_label_index=label_string_to_label_index))\n",
        "    return data_frame_multihot_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6803347f",
      "metadata": {
        "id": "6803347f"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "    df_train_noisy_multihot = pd.read_csv(DF_NOISY_MULTIHOT_PATH)\n",
        "except FileNotFoundError:\n",
        "    df_train_noisy = pd.read_csv(DF_NOISY_PATH)\n",
        "    df_train_noisy_multihot = encode_labels_to_multihot(df_train_noisy)\n",
        "    df_train_noisy_multihot.to_csv(DF_NOISY_MULTIHOT_PATH, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb53530a",
      "metadata": {
        "id": "bb53530a"
      },
      "source": [
        "Generate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13111772",
      "metadata": {
        "id": "13111772"
      },
      "source": [
        "### 3.1 Funciones de Preprocesamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dce3dfa",
      "metadata": {
        "id": "7dce3dfa"
      },
      "source": [
        "Load and Decode audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0fb372ed",
      "metadata": {
        "id": "0fb372ed"
      },
      "outputs": [],
      "source": [
        "def load_and_decode_audio(file_path):\n",
        "    \"\"\"\n",
        "    Loads and decodes a WAV file usando solo TensorFlow operations.\n",
        "    \"\"\"\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    waveform, _ = tf.audio.decode_wav(audio_binary)\n",
        "    waveform = tf.squeeze(waveform, axis=-1)\n",
        "    return waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1104ac4",
      "metadata": {
        "id": "b1104ac4"
      },
      "source": [
        "Split audio clip in same-length windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "07dff46b",
      "metadata": {
        "id": "07dff46b"
      },
      "outputs": [],
      "source": [
        "def split_audio_in_windows(audio, window_length_seconds=WINDOW_LENGTH_SECONDS, window_step_seconds=WINDOW_STEP_SECONDS, sample_rate=SAMPLE_RATE):\n",
        "    window_samples = int(window_length_seconds * sample_rate)\n",
        "    hop_samples = int(window_step_seconds * sample_rate)\n",
        "\n",
        "    windows = tf.signal.frame(\n",
        "        signal=audio,\n",
        "        frame_length=window_samples,\n",
        "        frame_step=hop_samples,\n",
        "        pad_end=True,\n",
        "        pad_value=0.0\n",
        "    )\n",
        "    return windows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e551ca",
      "metadata": {
        "id": "48e551ca"
      },
      "source": [
        "Create window's spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "87331f08",
      "metadata": {
        "id": "87331f08"
      },
      "outputs": [],
      "source": [
        "def convert_audio_tensor_to_mel_spectogram(audio_tensor, sample_rate, frame_length=N_FFT, frame_step=HOP_LENGTH, window_fn=tf.signal.hann_window, plot=True):\n",
        "    stft = tf.signal.stft(audio_tensor, frame_length=frame_length, frame_step=frame_step, window_fn=window_fn, fft_length=N_FFT)\n",
        "    spectrogram = tf.abs(stft)\n",
        "\n",
        "    num_spectrogram_bins = tf.shape(stft)[-1]\n",
        "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(N_MELS,\n",
        "                                                                        num_spectrogram_bins,\n",
        "                                                                        sample_rate,\n",
        "                                                                        FMIN,\n",
        "                                                                        FMAX)\n",
        "\n",
        "    mel_spectrogram = tf.tensordot(spectrogram, linear_to_mel_weight_matrix, 1)\n",
        "\n",
        "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)\n",
        "\n",
        "    if plot:\n",
        "        plot_spectogram(log_mel_spectrogram, frame_step, sample_rate)\n",
        "\n",
        "\n",
        "    return log_mel_spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d09ec9f1",
      "metadata": {
        "id": "d09ec9f1"
      },
      "outputs": [],
      "source": [
        "def convert_multihot_string_to_multihot(labels_string):\n",
        "    numpy_data = np.array(labels_string, dtype=np.float32)\n",
        "    return tf.convert_to_tensor(numpy_data, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87ab5e5",
      "metadata": {
        "id": "b87ab5e5"
      },
      "source": [
        "Generate audio windows from file path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "544dba18",
      "metadata": {
        "id": "544dba18"
      },
      "outputs": [],
      "source": [
        "def generate_windows(file_path, label):\n",
        "    audio = load_and_decode_audio(file_path)\n",
        "    windows = split_audio_in_windows(audio)\n",
        "    spectrograms = tf.map_fn(\n",
        "        lambda x: convert_audio_tensor_to_mel_spectogram(x, plot=False, sample_rate=SAMPLE_RATE),\n",
        "        windows,\n",
        "        fn_output_signature=tf.float32\n",
        "    )\n",
        "\n",
        "    # Repeat the label for each spectrogram window\n",
        "    num_windows = tf.shape(spectrograms)[0]\n",
        "    labels = tf.repeat([label], repeats=num_windows, axis=0)\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices((spectrograms, labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074d0b4e",
      "metadata": {
        "id": "074d0b4e"
      },
      "source": [
        "### 3.2 Creación del Pipeline `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0df2c95",
      "metadata": {
        "id": "d0df2c95"
      },
      "outputs": [],
      "source": [
        "df_noisy_multihot = pd.read_csv(DF_NOISY_MULTIHOT_PATH)\n",
        "\n",
        "# Fix: Parse the string representation of the list into a list of floats\n",
        "# without using eval. Remove the brackets and split by space and comma.\n",
        "labels_array = np.array([\n",
        "    [float(item) for item in label.strip(\"[]\").replace(\",\", \" \").split()]\n",
        "    for label in df_noisy_multihot['labels']\n",
        "], dtype=np.float32)\n",
        "\n",
        "\n",
        "full_paths = [str(PATH_NOISY / file_name) for file_name in df_noisy_multihot['fname']]\n",
        "\n",
        "# (file_paths), (labels) ->  (file_path, label)\n",
        "initial_dataset = tf.data.Dataset.from_tensor_slices((full_paths, labels_array))\n",
        "\n",
        "# (file_path, label) -> (audio_clip, label) -> (window, label) -> (spectrogram, label)\n",
        "processed_dataset = initial_dataset.flat_map(generate_windows)\n",
        "\n",
        "# 4. Write the dataset to TFRecord files (Sharding)\n",
        "SHARDS = 16 # Split into 16 files\n",
        "output_dir = TFRECORD_NOISY_DIR\n",
        "tf.io.gfile.makedirs(str(output_dir))\n",
        "\n",
        "print(\"Writing TFRecord files...\")\n",
        "# Calculate items per shard to improve progress reporting accuracy\n",
        "items_per_shard = len(df_noisy_multihot) // SHARDS\n",
        "writer = None # Initialize writer outside the loop\n",
        "\n",
        "for i, (spectrogram, label) in enumerate(processed_dataset):\n",
        "    shard_index = i % SHARDS\n",
        "    output_path = str(output_dir / f'shard-{shard_index:02d}-of-{SHARDS:02d}.tfrecord')\n",
        "\n",
        "    # Use a new writer for the first item of each shard or if it's the very first item\n",
        "    if i == 0 or i % items_per_shard == 0:\n",
        "         if writer is not None:\n",
        "             writer.close()\n",
        "         print(f\"Writing to shard {shard_index}...\")\n",
        "         writer = tf.io.TFRecordWriter(output_path)\n",
        "\n",
        "    example = serialize_example(spectrogram, label)\n",
        "    writer.write(example)\n",
        "\n",
        "# Ensure the last writer is closed after the loop finishes\n",
        "if writer is not None:\n",
        "    writer.close()\n",
        "\n",
        "print(\"Finished writing all TFRecord files.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spectogram_dataset(folder_path, multihot_labels, batch_size=BATCH_SIZE, sample_rate=SAMPLE_RATE,\n",
        "                             frame_length=N_FFT, frame_step=HOP_LENGTH, window_fn=tf.signal.hann_window, shuffle=True):\n",
        "\n",
        "    # Fix: Parse the string representation of the list into a list of floats\n",
        "    # without using eval. Remove the brackets and split by space and comma.\n",
        "    labels_array = np.array([\n",
        "      [float(item) for item in label.strip(\"[]\").replace(\",\", \" \").split()]\n",
        "      for label in df_noisy_multihot['labels']\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "\n",
        "    full_paths = [str(PATH_NOISY / file_name) for file_name in df_noisy_multihot['fname']]\n",
        "\n",
        "    # (file_paths), (labels) ->  (file_path, label)\n",
        "    initial_dataset = tf.data.Dataset.from_tensor_slices((full_paths, labels_array))\n",
        "\n",
        "    print(\"Loading .wav audio files...\")\n",
        "    dataset = dataset.map(lambda file_path, labels: (load_audio_with_librosa_wrapper(file_path, sample_rate), labels),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    print(\"Splitting audio in windows...\")\n",
        "    dataset = dataset.map(lambda audio, labels: (split_audio_in_windows(audio), labels),\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    print(\"Flattening...\")\n",
        "    dataset = dataset.flat_map(\n",
        "        lambda windows, labels: tf.data.Dataset.from_tensor_slices((\n",
        "            windows,\n",
        "            tf.repeat([labels], tf.shape(windows)[0], axis=0)\n",
        "        ))\n",
        "    )\n",
        "\n",
        "    print(\"Converting audio tensor to mel spectrogram...\")\n",
        "    dataset = dataset.map(lambda window, labels: (convert_audio_tensor_to_mel_spectogram(window,\n",
        "                                                                                      sample_rate=sample_rate,\n",
        "                                                                                      frame_length=frame_length,\n",
        "                                                                                      frame_step=frame_step,\n",
        "                                                                                      window_fn=window_fn,\n",
        "                                                                                      plot=False), labels), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.map(lambda spec, labels: (tf.ensure_shape(spec, [59, 128]), tf.ensure_shape(labels, [80])))\n",
        "\n",
        "    print(\"Adding channel dimension to spectrogram...\")\n",
        "    dataset = dataset.map(lambda spectrogram, label: (\n",
        "        tf.expand_dims(spectrogram, axis=-1),  # (59, 128) -> (59, 128, 1)\n",
        "        label\n",
        "    ))\n",
        "\n",
        "    dataset = dataset.map(lambda spec, labels: (tf.ensure_shape(spec, [59, 128, 1]), tf.ensure_shape(labels, [80])))\n",
        "\n",
        "    if shuffle:\n",
        "        print(\"Shuffling...\")\n",
        "        dataset = dataset.shuffle(buffer_size=1000)\n",
        "\n",
        "    print(\"Batching...\")\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(lambda specs, labels: (tf.ensure_shape(specs, [None, 59, 128, 1]), tf.ensure_shape(labels, [None, 80])))\n",
        "\n",
        "\n",
        "    print(\"Prefetching...\")\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "NDezUMarY2P2"
      },
      "id": "NDezUMarY2P2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d66fed54",
      "metadata": {
        "id": "d66fed54"
      },
      "outputs": [],
      "source": [
        "dataset_size = tf.data.experimental.cardinality(train_noisy_dataset)\n",
        "print(f\"Dataset size: {dataset_size}\")\n",
        "\n",
        "# Look at one batch to verify pipeline works\n",
        "for spectogram, labels in train_noisy_dataset.take(1):\n",
        "    print(f\"Spectogram shape: {spectogram.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c805164",
      "metadata": {
        "id": "5c805164"
      },
      "source": [
        "---\n",
        "## 4. Métrica de Evaluación (`lwlrap`)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4cd5dc",
      "metadata": {
        "id": "bb4cd5dc"
      },
      "source": [
        "La métrica lwlrap se implementa usando el módulo lwlrap_metric.py que contiene una implementación optimizada para TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bda4b17",
      "metadata": {
        "id": "4bda4b17"
      },
      "outputs": [],
      "source": [
        "#from lwlrap_metric import LwlrapMetric\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Métrica Label-Weighted Label-Ranking Average Precision (lwlrap) para TensorFlow/Keras.\n",
        "Contiene la implementación de la métrica para usar durante el entrenamiento y funciones\n",
        "auxiliares para evaluación post-entrenamiento.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import sklearn.metrics\n",
        "\n",
        "#---- PARTE 1: MÉTRICA PARA TENSORFLOW/KERAS ----------------------------------#\n",
        "\n",
        "@tf.function\n",
        "def _tf_one_sample_positive_class_precisions(scores, truth):\n",
        "    \"\"\"\n",
        "    Versión en TensorFlow para calcular las precisiones de las clases positivas\n",
        "    para una única muestra.\n",
        "    \"\"\"\n",
        "    num_classes = tf.shape(scores)[0]\n",
        "    pos_class_indices = tf.where(truth > 0)[:, 0]\n",
        "\n",
        "    if tf.size(pos_class_indices) == 0:\n",
        "        return pos_class_indices, tf.constant([], dtype=tf.float32)\n",
        "\n",
        "    retrieved_classes = tf.argsort(scores, direction='DESCENDING')\n",
        "\n",
        "    class_rankings = tf.scatter_nd(\n",
        "        tf.expand_dims(retrieved_classes, 1),\n",
        "        tf.range(num_classes, dtype=tf.int32),\n",
        "        [num_classes]\n",
        "    )\n",
        "\n",
        "    retrieved_class_true = tf.scatter_nd(\n",
        "        tf.expand_dims(tf.gather(class_rankings, pos_class_indices), 1),\n",
        "        tf.ones(tf.size(pos_class_indices), dtype=tf.bool),\n",
        "        [num_classes]\n",
        "    )\n",
        "\n",
        "    retrieved_cumulative_hits = tf.cumsum(tf.cast(retrieved_class_true, tf.float32))\n",
        "\n",
        "    rankings_pos_classes = tf.gather(class_rankings, pos_class_indices)\n",
        "    precision_at_hits = (\n",
        "        tf.gather(retrieved_cumulative_hits, rankings_pos_classes) /\n",
        "        (tf.cast(rankings_pos_classes, tf.float32) + 1.0)\n",
        "    )\n",
        "\n",
        "    return pos_class_indices, precision_at_hits\n",
        "\n",
        "class LwlrapMetric(keras.metrics.Metric):\n",
        "    \"\"\"\n",
        "    Métrica lwlrap para TensorFlow/Keras.\n",
        "    Funciona con operaciones de TensorFlow para ser compatible con el modo graph.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, name='lwlrap', **kwargs):\n",
        "        super(LwlrapMetric, self).__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.per_class_cumulative_precision = self.add_weight(\n",
        "            name='per_class_cumulative_precision',\n",
        "            shape=(num_classes,),\n",
        "            initializer='zeros',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "        self.per_class_cumulative_count = self.add_weight(\n",
        "            name='per_class_cumulative_count',\n",
        "            shape=(num_classes,),\n",
        "            initializer='zeros',\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "        def process_sample(sample_data):\n",
        "            truth, scores = sample_data\n",
        "            pos_class_indices, precision_at_hits = _tf_one_sample_positive_class_precisions(scores, truth)\n",
        "            precision_update = tf.scatter_nd(\n",
        "                tf.expand_dims(pos_class_indices, 1),\n",
        "                precision_at_hits,\n",
        "                [self.num_classes]\n",
        "            )\n",
        "            count_update = tf.scatter_nd(\n",
        "                tf.expand_dims(pos_class_indices, 1),\n",
        "                tf.ones(tf.size(pos_class_indices), dtype=tf.float32),\n",
        "                [self.num_classes]\n",
        "            )\n",
        "            return precision_update, count_update\n",
        "\n",
        "        precision_updates, count_updates = tf.map_fn(\n",
        "            process_sample,\n",
        "            (y_true, y_pred),\n",
        "            fn_output_signature=(\n",
        "                tf.TensorSpec([self.num_classes], dtype=tf.float32),\n",
        "                tf.TensorSpec([self.num_classes], dtype=tf.float32)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.per_class_cumulative_precision.assign_add(tf.reduce_sum(precision_updates, axis=0))\n",
        "        self.per_class_cumulative_count.assign_add(tf.reduce_sum(count_updates, axis=0))\n",
        "\n",
        "    def result(self):\n",
        "        per_class_lwlrap = tf.divide(\n",
        "            self.per_class_cumulative_precision,\n",
        "            tf.maximum(self.per_class_cumulative_count, 1.0)\n",
        "        )\n",
        "        total_count = tf.reduce_sum(self.per_class_cumulative_count)\n",
        "        weight_per_class = tf.divide(\n",
        "            self.per_class_cumulative_count,\n",
        "            tf.maximum(total_count, 1.0)\n",
        "        )\n",
        "        overall_lwlrap = tf.reduce_sum(per_class_lwlrap * weight_per_class)\n",
        "        return overall_lwlrap\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.per_class_cumulative_precision.assign(tf.zeros_like(self.per_class_cumulative_precision))\n",
        "        self.per_class_cumulative_count.assign(tf.zeros_like(self.per_class_cumulative_count))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# PARTE 2: FUNCIONES DE NUMPY/SKLEARN (OPCIONAL, ÚTIL PARA ANÁLISIS POST-ENTRENAMIENTO)\n",
        "# ------------------------------------------------------------------------------------\n",
        "\n",
        "def calculate_overall_lwlrap_sklearn(truth, scores):\n",
        "    \"\"\"Calcula el lwlrap general usando la implementación de referencia de sklearn.\"\"\"\n",
        "    sample_weight = np.sum(truth > 0, axis=1)\n",
        "    nonzero_weight_sample_indices = np.flatnonzero(sample_weight > 0)\n",
        "    if len(nonzero_weight_sample_indices) == 0:\n",
        "        return 0.0\n",
        "    overall_lwlrap = sklearn.metrics.label_ranking_average_precision_score(\n",
        "        truth[nonzero_weight_sample_indices, :] > 0,\n",
        "        scores[nonzero_weight_sample_indices, :],\n",
        "        sample_weight=sample_weight[nonzero_weight_sample_indices]\n",
        "    )\n",
        "    return overall_lwlrap\n",
        "\n",
        "\n",
        "# Definir la métrica lwlrap para usar en Keras\n",
        "lwlrap_metric = LwlrapMetric(num_classes=NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d930de",
      "metadata": {
        "id": "e8d930de"
      },
      "source": [
        "---\n",
        "## 5. Configuración de Experimentación con `Comet.ml`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3ffdc1d",
      "metadata": {
        "id": "b3ffdc1d"
      },
      "source": [
        "### 5.1 Conexión con Comet.ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3053ebc2",
      "metadata": {
        "id": "3053ebc2"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    api_key = userdata.get('COMET_API_KEY')\n",
        "    comet_ml.login(api_key=api_key)\n",
        "    print(\"Conexión con Comet.ml exitosa.\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"⚠️ Clave API de Comet no encontrada. Guardala como 'COMET_API_KEY' en los secrets de Colab (a la izquierda).\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al conectar con Comet.ml: {e}\")\n",
        "\n",
        "# Esta función ayuda a iniciar cada experimento de forma limpia.\n",
        "# Revisar esto\n",
        "def create_comet_experiment(name, tags, params):\n",
        "    \"\"\"Crea y configura un nuevo experimento en Comet.\"\"\"\n",
        "    experiment = comet_ml.Experiment(\n",
        "        project_name=\"freesound-audio-tagging-2019\",\n",
        "        auto_metric_logging=True,\n",
        "        auto_param_logging=True\n",
        "    )\n",
        "    experiment.set_name(name)\n",
        "    experiment.add_tags(tags)\n",
        "    experiment.log_parameters(params)\n",
        "    return experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381ee2bb",
      "metadata": {
        "id": "381ee2bb"
      },
      "source": [
        "---\n",
        "## 6. Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c321cd4",
      "metadata": {
        "id": "0c321cd4"
      },
      "source": [
        "### 6.1 Experimento 1: Línea Base"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db7fda8",
      "metadata": {
        "id": "1db7fda8"
      },
      "source": [
        "**Hipótesis:** Una CNN simple, entrenada solo con los datos curados, puede aprender a diferenciar algunas de las clases más comunes.\n",
        "\n",
        "**Configuración:**\n",
        "-   **Datos:** Conjunto curado (80/20 train/val).\n",
        "-   **Arquitectura:** CNN 2D simple (2 bloques Conv + Pool).\n",
        "-   **Pérdida:** `BinaryCrossentropy` (adecuada para multi-label).\n",
        "-   **Métricas:** `lwlrap`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c6326d",
      "metadata": {
        "id": "b8c6326d"
      },
      "source": [
        "#### 6.1.1 Definición del Modelo Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e96fb5",
      "metadata": {
        "id": "50e96fb5"
      },
      "outputs": [],
      "source": [
        "def build_baseline_model(input_shape, num_classes):\n",
        "    \"\"\"Construye un modelo CNN 2D simple.\"\"\"\n",
        "    # Add a channel dimension to the input shape for Conv2D\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(input_shape)),\n",
        "\n",
        "        # Completar la arquitectura. Ver `simple_audio.py`o pensar otra\n",
        "        # Un buen punto de partida seria:\n",
        "        # Bloque 1: Conv2D(32, (3,3), activation='relu') -> MaxPool2D()\n",
        "        # Bloque 2: Conv2D(64, (3,3), activation='relu') -> MaxPool2D()\n",
        "        # Aplanado: GlobalAveragePooling2D() o Flatten()\n",
        "        # Clasificador: Dense(128, activation='relu') -> Dropout(0.5) -> Dense(num_classes, activation='sigmoid')\n",
        "\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "        tf.keras.layers.Dense(num_classes, activation='sigmoid') # Sigmoid para multi-label\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Construimos y compilamos el modelo\n",
        "INPUT_SHAPE = [59, 128, 1]\n",
        "baseline_model = build_baseline_model(INPUT_SHAPE, NUM_CLASSES)\n",
        "baseline_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[lwlrap_metric]\n",
        ")\n",
        "baseline_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa33545",
      "metadata": {
        "id": "4fa33545"
      },
      "source": [
        "#### 6.1.2 Entrenamiento del Modelo Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dUaza8beGabK",
      "metadata": {
        "id": "dUaza8beGabK"
      },
      "outputs": [],
      "source": [
        "# Profile your data pipeline\n",
        "import time\n",
        "\n",
        "print(\"=== DATA PIPELINE PROFILING ===\")\n",
        "dataset_iter = iter(train_noisy_dataset)\n",
        "\n",
        "# Time data loading\n",
        "start_time = time.time()\n",
        "for i in range(10):\n",
        "    batch = next(dataset_iter)\n",
        "    if i == 0:\n",
        "        print(f\"Batch shape: {batch[0].shape}\")\n",
        "    print(f\"Batch {i+1}: {(time.time() - start_time)*1000:.1f}ms\")\n",
        "    start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cba99b2f",
      "metadata": {
        "id": "cba99b2f"
      },
      "outputs": [],
      "source": [
        "# Configuración del experimento en Comet\n",
        "exp_baseline_params = {\n",
        "    'model_type': 'baseline_cnn',\n",
        "    'dataset': 'training_only',\n",
        "    'epochs': 20,\n",
        "    'batch_size': 64\n",
        "}\n",
        "exp_baseline = create_comet_experiment(\n",
        "    name=\"Baseline CNN\",\n",
        "    tags=[\"baseline\", \"cnn\", \"training-only\"],\n",
        "    params=exp_baseline_params\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_lwlrap', patience=5, mode='max', restore_best_weights=True)\n",
        "comet_callback = exp_baseline.get_callback('keras')\n",
        "\n",
        "# Entrenamiento\n",
        "history_baseline = baseline_model.fit(\n",
        "    train_noisy_dataset,\n",
        "    epochs=exp_baseline_params['epochs'],\n",
        "    callbacks=[comet_callback]\n",
        ")\n",
        "\n",
        "# Fin\n",
        "exp_baseline.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e6c2d74",
      "metadata": {
        "id": "4e6c2d74"
      },
      "source": [
        "### 6.2 Experimento 2: Mejoras sobre la Línea Base"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2551800d",
      "metadata": {
        "id": "2551800d"
      },
      "source": [
        "**Hipótesis:** Añadir regularización (Batch Norm, Dropout más robusto) y Data Augmentation mejorará la generalización y el score `lwlrap`.\n",
        "\n",
        "**Configuración:**\n",
        "-   **Datos:** Mismos datos, pero con aumento de datos en el pipeline.\n",
        "-   **Arquitectura:** CNN mejorada (más profunda, con `BatchNormalization`).\n",
        "-   **Técnicas:** `SpecAugment` (enmascaramiento en frecuencia y tiempo)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fbf5329",
      "metadata": {
        "id": "3fbf5329"
      },
      "source": [
        "#### 6.2.1 Data Augmentation (SpecAugment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f195dd",
      "metadata": {
        "id": "c7f195dd"
      },
      "outputs": [],
      "source": [
        "# Completar: Implementar SpecAugment.\n",
        "# Esqueleto:\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "@tf.function\n",
        "def augment_spectrogram(spectrogram, label):\n",
        "    \"\"\"Aplica SpecAugment a un espectrograma.\"\"\"\n",
        "    # Enmascaramiento en el tiempo\n",
        "    spectrogram = tfa.image.random_cutout(\n",
        "        images=tf.expand_dims(spectrogram, 0), # Requiere un batch\n",
        "        mask_size=(10, 20), # (alto, ancho) del recorte -> (freq, tiempo)\n",
        "    )[0] # Deshacer el batch\n",
        "\n",
        "    # Enmascaramiento en frecuencia\n",
        "    spectrogram = tfa.image.random_cutout(\n",
        "        images=tf.expand_dims(spectrogram, 0),\n",
        "        mask_size=(20, 10),\n",
        "    )[0]\n",
        "\n",
        "    return spectrogram, label\n",
        "\n",
        "# Creamos un nuevo pipeline de entrenamiento con augmentation\n",
        "train_ds_aug = train_ds.map(augment_spectrogram, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ffe689",
      "metadata": {
        "id": "40ffe689"
      },
      "source": [
        "#### 6.2.2 Definición del Modelo Mejorado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95da6d93",
      "metadata": {
        "id": "95da6d93"
      },
      "outputs": [],
      "source": [
        "def build_improved_model(input_shape, num_classes):\n",
        "    \"\"\"Construye una CNN más robusta con BatchNormalization.\"\"\"\n",
        "    # Mejorar el modelo base. Probar el patrón Conv -> BatchNorm -> ReLU -> Pool.\n",
        "    # Se puede añadir más bloques para hacerlo más profundo.\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=input_shape),\n",
        "        # Bloque 1\n",
        "        tf.keras.layers.Conv2D(32, (3,3), padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "        # Bloque 2\n",
        "        tf.keras.layers.Conv2D(64, (3,3), padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "        # Bloque 3\n",
        "        tf.keras.layers.Conv2D(128, (3,3), padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.Activation('relu'),\n",
        "\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Compilamos\n",
        "improved_model = build_improved_model(INPUT_SHAPE, NUM_CLASSES)\n",
        "improved_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[lwlrap_tf]\n",
        ")\n",
        "improved_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d8c7b5",
      "metadata": {
        "id": "d5d8c7b5"
      },
      "source": [
        "#### 6.2.3 Entrenamiento del Modelo Mejorado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e14d46",
      "metadata": {
        "id": "b7e14d46"
      },
      "outputs": [],
      "source": [
        "# Configuración del experimento\n",
        "exp_improved_params = {\n",
        "    'model_type': 'improved_cnn',\n",
        "    'dataset': 'curated_only',\n",
        "    'augmentation': 'specaugment',\n",
        "    'epochs': 40, # Más épocas por la regularización\n",
        "    'batch_size': 32\n",
        "}\n",
        "exp_improved = create_comet_experiment(\n",
        "    name=\"Improved CNN + SpecAugment\",\n",
        "    tags=[\"improved\", \"cnn\", \"specaugment\"],\n",
        "    params=exp_improved_params\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stopping_imp = tf.keras.callbacks.EarlyStopping(monitor='val_lwlrap_tf', patience=7, mode='max', restore_best_weights=True)\n",
        "comet_callback_imp = exp_improved.get_callback('keras')\n",
        "\n",
        "# Entrenamiento\n",
        "history_improved = improved_model.fit(\n",
        "    train_ds_aug, # Usamos el dataset con augmentation\n",
        "    epochs=exp_improved_params['epochs'],\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[early_stopping_imp, comet_callback_imp]\n",
        ")\n",
        "exp_improved.end()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd08de8",
      "metadata": {
        "id": "9bd08de8"
      },
      "source": [
        "### 6.3 Experimento 3: Enfoque Avanzado"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e5fca9e",
      "metadata": {
        "id": "0e5fca9e"
      },
      "source": [
        "**Hipótesis:** Aprovechar un modelo pre-entrenado (Transfer Learning) o los datos ruidosos puede mejorar significativamente el rendimiento, ya que nuestro conjunto curado es pequeño."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490ffe6a",
      "metadata": {
        "id": "490ffe6a"
      },
      "source": [
        "**Opción A: Transferencia de Aprendizaje (Transfer Learning)**\n",
        "Podemos usar un modelo pre-entrenado en ImageNet, como `EfficientNetB0`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a1fd8b",
      "metadata": {
        "id": "b2a1fd8b"
      },
      "source": [
        "#### 6.3.1 Definición del Modelo con Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ca215c",
      "metadata": {
        "id": "80ca215c"
      },
      "outputs": [],
      "source": [
        "def build_transfer_model(input_shape, num_classes):\n",
        "    \"\"\"Construye un modelo usando EfficientNetB0 pre-entrenado.\"\"\"\n",
        "    # Los modelos de ImageNet esperan 3 canales. Duplicamos nuestro canal único.\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Concatenate(axis=-1)([inputs, inputs, inputs])\n",
        "\n",
        "    # Cargamos el modelo base sin el clasificador\n",
        "    base_model = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=x\n",
        "    )\n",
        "\n",
        "    # Congelamos el modelo base para no destruir los pesos pre-entrenados\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Añadimos nuestro propio clasificador\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model, base_model\n",
        "\n",
        "# El entrenamiento con Transfer Learning se suele hacer en dos fases:\n",
        "# 1. Entrenar solo el clasificador nuevo con el modelo base congelado.\n",
        "# 2. Descongelar algunas capas del modelo base y re-entrenar (fine-tuning) con una tasa de aprendizaje muy baja.\n",
        "\n",
        "# --- FASE 1: ENTRENAMIENTO DEL CLASIFICADOR ---\n",
        "transfer_model, base_model = build_transfer_model(INPUT_SHAPE, NUM_CLASSES)\n",
        "transfer_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[lwlrap_tf]\n",
        ")\n",
        "transfer_model.summary()\n",
        "\n",
        "# Entrenamiento fase 1... (similar a los anteriores)\n",
        "\n",
        "# --- FASE 2: FINE-TUNING (Opcional pero recomendado) ---\n",
        "# base_model.trainable = True\n",
        "# # Descongelar solo las últimas capas es una buena práctica\n",
        "# for layer in base_model.layers[:-20]:\n",
        "#     layer.trainable = False\n",
        "#\n",
        "# transfer_model.compile(\n",
        "#     optimizer=tf.keras.optimizers.Adam(1e-5), # Tasa de aprendizaje muy baja\n",
        "#     loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "#     metrics=[lwlrap_tf]\n",
        "# )\n",
        "#\n",
        "# Entrenamiento fase 2..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4745e97",
      "metadata": {
        "id": "b4745e97"
      },
      "source": [
        "**Opción B: Uso de Datos Ruidosos (Pre-training / Fine-tuning)**\n",
        "Esta es una estrategia poderosa.\n",
        "1.  **Pre-entrenamiento:** Entrenar un modelo en el gran conjunto de datos `ruidoso`. El modelo aprenderá características de audio generales, a pesar de las etiquetas imperfectas.\n",
        "2.  **Fine-tuning:** Tomar el modelo pre-entrenado y continuar su entrenamiento en el conjunto `curado`, que es más pequeño pero de alta calidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b543284",
      "metadata": {
        "id": "3b543284"
      },
      "outputs": [],
      "source": [
        "# 1. Crear un `tf.data.Dataset` para el conjunto ruidoso (`df_noisy`).\n",
        "# 2. Entrenar uno de los modelos (ej. el `improved_model`) en este dataset ruidoso por varias épocas.\n",
        "# 3. Guardar los pesos de este modelo.\n",
        "# 4. Cargar esos pesos en un nuevo modelo y entrenarlo en el dataset curado (`train_ds`).\n",
        "# ¡No olvidar registrar ambos pasos en Comet!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7033475",
      "metadata": {
        "id": "b7033475"
      },
      "source": [
        "## 7. Generación de la Submission para Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c2f9bd",
      "metadata": {
        "id": "f4c2f9bd"
      },
      "source": [
        "Una vez que tengamos **el mejor modelo**, lo usamos para predecir las etiquetas en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63684928",
      "metadata": {
        "id": "63684928"
      },
      "source": [
        "### 7.1 Creación del Pipeline de Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e5a7b11",
      "metadata": {
        "id": "2e5a7b11"
      },
      "outputs": [],
      "source": [
        "def create_test_dataset(df, path_prefix):\n",
        "    filepaths = [str(path_prefix / fname) for fname in df['fname']]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(filepaths)\n",
        "\n",
        "    # El preprocesamiento debe ser idéntico al de validación\n",
        "    def preprocess_test_file(filepath):\n",
        "        waveform = load_audio(filepath)\n",
        "        waveform = pad_or_truncate(waveform)\n",
        "        spectrogram = to_mel_spectrogram(spectrogram)\n",
        "        return spectrogram\n",
        "\n",
        "    dataset = dataset.map(preprocess_test_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(64) # Usar un batch size grande para acelerar la predicción\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "test_ds = create_test_dataset(df_submission, PATH_TEST)\n",
        "\n",
        "# Usamos nuestro mejor modelo para predecir\n",
        "# A A A A A A A   ¡¡Reemplazar `improved_model` por el mejor!!\n",
        "best_model = improved_model\n",
        "predictions = best_model.predict(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba54c67",
      "metadata": {
        "id": "7ba54c67"
      },
      "source": [
        "### 7.2 Formateo y Guardado del Archivo de Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4a5003e",
      "metadata": {
        "id": "a4a5003e"
      },
      "outputs": [],
      "source": [
        "# Creamos el dataframe de submission con las predicciones\n",
        "submission_df = pd.DataFrame(predictions, columns=LABELS)\n",
        "submission_df['fname'] = df_submission['fname']\n",
        "\n",
        "# Reordenamos las columnas para que coincida con el formato de `sample_submission.csv`\n",
        "submission_df = submission_df[df_submission.columns]\n",
        "\n",
        "# Guardamos el archivo\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "print(\"Archivo submission.csv creado.\")\n",
        "submission_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e11b990",
      "metadata": {
        "id": "2e11b990"
      },
      "source": [
        "## 8. Comparación y Análisis Final"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6195d41",
      "metadata": {
        "id": "f6195d41"
      },
      "source": [
        "Acá es donde reflexionamos sobre nuestros resultados :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76234848",
      "metadata": {
        "id": "76234848"
      },
      "source": [
        "## 8.1 Tabla Comparativa de Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e11ce5",
      "metadata": {
        "id": "c9e11ce5"
      },
      "outputs": [],
      "source": [
        "# Ir al dashboard de Comet.ml, buscar los scores de `val_lwlrap_tf` de cada experimento\n",
        "# y completar esta tabla:\n",
        "\n",
        "results_data = {\n",
        "    'Experimento': ['1. Línea Base', '2. Mejorado + Augment', '3. Avanzado (TL/Ruidoso)'],\n",
        "    'Mejor val_lwlrap': [0.0, 0.0, 0.0], # <-- COMPLETAR ACÁ\n",
        "    'Notas': [\n",
        "        'CNN simple, solo datos curados.',\n",
        "        'CNN más profunda con BN y SpecAugment.',\n",
        "        'Resultados del mejor enfoque avanzado.'\n",
        "    ]\n",
        "}\n",
        "results_df = pd.DataFrame(results_data)\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5eed279",
      "metadata": {
        "id": "b5eed279"
      },
      "source": [
        "¿Qué conclusiones podemos sacar?\n",
        "- ¿Funcionó la data augmentation?\n",
        "- ¿Cuál fue el impacto de una arquitectura más profunda?\n",
        "- Si probamos un enfoque avanzado, ¿cuál fue el beneficio?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}